{
    "videos": [
        {
            "name": "Fluid Simulations with Blender and Wavelet Turbulence",
            "url": "https://www.youtube.com/watch?v=5xLSbj5SsSE",
            "date": "2015-07-28",
            "tags": [],
            "papers": ["http://www.cs.cornell.edu/~tedkim/wturb/wavelet_turbulence.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Capturing Waves of Light With Femto-photography",
            "url": "https://www.youtube.com/watch?v=TRNUTN01SEg",
            "date": "2015-08-08",
            "tags": [],
            "papers": ["http://dspace.mit.edu/openaccess-disseminate/1721.1/82039"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Artificial Neural Networks and Deep Learning",
            "url": "https://www.youtube.com/watch?v=rCWTOOgVXyE",
            "papers": [],
            "date": "2015-08-14"
        },
        {
            "name": "Simulating Breaking Glass",
            "url": "https://www.youtube.com/watch?v=A7Gut679I-o",
            "date": "2015-08-22",
            "tags": [],
            "papers": ["http://graphics.berkeley.edu/papers/Pfaff-ATC-2014-07/Pfaff-ATC-2014-07.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Time Lapse Videos From Community Photos",
            "url": "https://www.youtube.com/watch?v=UePDRN94C8c",
            "date": "2015-08-25",
            "tags": [],
            "papers": ["http://grail.cs.washington.edu/projects/timelapse/TimelapseMiningSIGGRAPH15.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Deep Neural Network Learns Van Gogh's Art",
            "url": "https://www.youtube.com/watch?v=-R9bJGNHltQ",
            "date": "2015-08-29",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1508.06576v1.pdf"],
            "implementations": ["https://github.com/jcjohnson/neural-style"],
            "transcript": []
        },
        {
            "name": "Hydrographic Printing (in 3D)",
            "url": "https://www.youtube.com/watch?v=kLnG073NYtw",
            "date": "2015-08-31",
            "tags": [],
            "papers": ["http://www.cs.columbia.edu/~cxz/publications/hydrographics.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Digital Creatures Learn To Walk",
            "url": "https://www.youtube.com/watch?v=kQ2bqz3HPJE",
            "date": "2015-09-08",
            "tags": [],
            "papers": ["http://www.goatstream.com/research/papers/SA2013/SA2013.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": " Manipulating Photorealistic Renderings",
            "url": "https://www.youtube.com/watch?v=L7MOeQw47BM",
            "date": "2015-09-11",
            "tags": [],
            "papers": ["https://cg.ivd.kit.edu/publications/p2013/PSMPBLT/PSMPBLT.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Adaptive Fluid Simulations",
            "url": "https://www.youtube.com/watch?v=dH1s49-lrBk",
            "date": "2015-09-21",
            "tags": [],
            "papers": ["http://pub.ist.ac.at/group_wojtan/projects/2013_Ando_HALSoTM/download/tetflip_fixed.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Building Bridges With Flying Machines",
            "url": "https://www.youtube.com/watch?v=SmyiKmfnbhc",
            "date": "2015-09-27",
            "tags": [],
            "papers": ["http://flyingmachinearena.org/wp-content/publications/2013/augIROS13.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Reconstructing Sound From Vibrations",
            "url": "https://www.youtube.com/watch?v=2i1hrywDwPo",
            "date": "2015-09-30",
            "tags": [],
            "papers": ["http://people.csail.mit.edu/mrub/papers/VisualMic_SIGGRAPH2014.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Creating Photographs Using Deep Learning",
            "url": "https://www.youtube.com/watch?v=HOLoPgTzV6g",
            "date": "2015-10-03",
            "tags": [],
            "papers": ["http://research.microsoft.com/en-us/um/people/yuedong/project/neuralibr/neuralibr.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Adaptive Cloth Simulations",
            "url": "https://www.youtube.com/watch?v=LU3pdWTD4Rw",
            "date": "2015-10-06",
            "tags": [],
            "papers": ["http://graphics.berkeley.edu/papers/Narain-AAR-2012-11/Narain-AAR-2012-11.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Synthesizing Sound From Collisions",
            "url": "https://www.youtube.com/watch?v=rskdLEl05KI",
            "date": "2015-10-11",
            "tags": [],
            "papers": ["http://www.cs.cornell.edu/projects/Sound/mc/ModalContactSound2011.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Metropolis Light Transport",
            "url": "https://www.youtube.com/watch?v=f0Uzit_-h3M",
            "date": "2015-10-14",
            "tags": [],
            "papers": ["http://graphics.stanford.edu/papers/metro/metro.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "3D Printing a Glockenspiel",
            "url": "https://www.youtube.com/watch?v=2kOCTf8jIik",
            "date": "2015-10-17",
            "tags": [],
            "papers": ["http://people.seas.harvard.edu/~gaurav/papers/cdmcs_sa_2015/cdmcs_sa_2015.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Modeling Colliding and Merging Fluids",
            "url": "https://www.youtube.com/watch?v=uj8b5mu0P7Y",
            "date": "2015-10-20",
            "tags": [],
            "papers": ["http://www.cs.columbia.edu/cg/multitracker/multitracker.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Recurrent Neural Network Writes Music and Shakespeare Novels",
            "url": "https://www.youtube.com/watch?v=Jkkjy7dVdaY",
            "date": "2015-10-23",
            "tags": [],
            "papers": ["http://karpathy.github.io/2015/05/21/rnn-effectiveness/"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Gradients, Poisson's Equation and Light Transport",
            "url": "https://www.youtube.com/watch?v=sSnDTPjfBYU",
            "date": "2015-10-26",
            "tags": [],
            "papers": ["https://mediatech.aalto.fi/publications/graphics/GPT/kettunen2015siggraph_paper.pdf", "https://mediatech.aalto.fi/publications/graphics/GMLT/lehtinen2013siggraph_paper.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Real-Time Facial Expression Transfer",
            "url": "https://www.youtube.com/watch?v=mkI6qfpEJmI",
            "date": "2015-10-30",
            "tags": [],
            "papers": ["http://graphics.stanford.edu/~niessner/papers/2015/10face/thies2015realtime.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Automatic Lecture Notes From Videos",
            "url": "https://www.youtube.com/watch?v=8xjTtE3JCDw",
            "date": "2015-11-03",
            "tags": [],
            "papers": ["http://web.mit.edu/hishin/www/paper.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Recurrent Neural Network Writes Sentences About Images",
            "url": "https://www.youtube.com/watch?v=e-WB4lfg30M",
            "date": "2015-11-07",
            "tags": [],
            "papers": ["http://cs.stanford.edu/people/karpathy/cvpr2015.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "How Does Deep Learning Work?",
            "url": "https://www.youtube.com/watch?v=He4t7Zekob0",
            "date": "2015-11-11",
            "tags": [],
            "papers": ["http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A neural network is a very loose model of the human brain that we can program in a computer, or it's perhaps more appropriate to say that it is inspired by our knowledge of the inner workings of a human brain. Now, let's note that artificial neural networks have been studied for decades by experts, and the goal here is not to show all aspects, but one intuitive, graphical aspect that is really cool and easy to understand. Take a look at these curves on a plane. These curves are a collection of points, and these points you can imagine as images, sounds, or any kind of input data that we try to learn. The red and the blue curves represent two different classes - the red can mean images of trains, and the blue, for instance, images of bunnies. Now, after we have trained the network from this limited data, which is basically a bunch of images of of trains and bunnies, we will get new points on this plane, new images, and we would like to know whether this new image looks like a train or a bunny. This is what the algorithm has to find out. And this we call a classification problem, to which a simple and bad solution would be simply cutting the plane in half with a line. Images belonging to the red regions will be classified as the red class, and the blue regions as the blue class. Now, as you see, the red region cuts into the blue curve, which means that some trains would be misclassified as bunnies. It seems that if we look at the problem from this angle, we cannot separate the two classes perfectly with a straight line. However, if we use a simple neural network, it will give us this result. Hey! But that's cheating, we were talking about straight lines. This is anything but a straight line. A key concept of neural networks is that they create an inner representation of the data model and try to solve the problem in that space. What this intuitively means, is that the algorithm will start transforming and warping these curves, where their shapes start changing, and it finds, that if we do well with this warping step, we can actually draw a line to separate these two classes. After we undo this warping and transform the line here back to the original problem, it will look like a curve. Really cool, isn't it? So these are lines, only in a different representation of the problem. Who said that the original representation is the best way to solve a problem? Take a look at this example with these entangled spirals. Can we separate these with a line? Not a chance. But the answer is - not a chance with this representation. But if one starts warping them correctly, there will be states where they can easily be separated. However, there are rules in this game - for instance, one cannot just rip out one of the spirals here and put it somewhere else. These transformations have to be homeomorphisms, which is a term that mathematicians like to use - it intuitively means that that the warpings are not too crazy - meaning that we don't tear apart important structures, and as they remain intact, the warped solution is still meaningful with respect to the original problem. Now comes the deep learning part. Deep learning means that the neural network has multiple of these hidden layers and can therefore create much more effective inner representations of the data. From an earlier episode, we've seen in an image recognition task that as we go further and further into the layers, first we'll see an edge detector, and as a combination of edges, object parts emerge, and in the later layers, a combination of object parts create object models. Let's take a look at this example. We have a bullseye here if you will, and you can see that the network is trying to warp this to separate it with a line, but in vain. However, if we have a deep neural network, we have more degrees of freedom, more directions and possibilities to warp this data. And if you think intuitively, if this were a piece of paper, you could put your finger behind the red zone and push it in, making it possible to separate the two regions with a line. Let's take a look at a 1 dimensional example to see better what's going on. This line is the 1D equivalent of the original problem, and you see that the problem becomes quite trivial if we have the freedom to do this transformation. We can easily encounter cases where the data is very severely tangled and we don't now how good our best solution can be. There is a very heavily academic subfield of mathematics, called knot theory, which is the study of tangling and untangling objects. It is subject to a lot of snarky comments for not being well, too exciting or useful. What is really mind blowing is that knot theory can actually help us study these kinds of problems and it may ultimately end up being useful for recognizing traffic signs and designing self-driving cars. Now, it's time to get our hands dirty! Let's run a neural network on this dataset. If we use a low number of neurons and one layer, you can see that it is trying ferociously, but we know that it is going to be a fruitless endeavor. Upon increasing the number of neurons, magic happens. And we now know exactly why! Yeah! Thanks so much for watching and for your generous support. I feel really privileged to have supporters like you Fellow Scholars. Thank you, and I'll see you next time!"
        },
        {
            "name": "Cryptography, Perfect Secrecy and One Time Pads",
            "url": "https://www.youtube.com/watch?v=Q-XKOPNIDAg",
            "date": "2015-11-15",
            "tags": [],
            "papers": ["http://math.boisestate.edu/~liljanab/Math509Spring10/vernam.pdf"],
            "implementations": [],
            "transcript": []
        },
        {
            "name": "Terrain Traversal with Reinforcement Learning",
            "url": "https://www.youtube.com/watch?v=_yjHPu1aYCY",
            "date": "2015-11-18",
            "tags": [],
            "papers": ["http://www.cs.ubc.ca/~van/papers/2015-TOG-terrainRL/2015-TOG-terrainRL.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér Reinforcement learning is a technique that can learn how to play computer games, or any kind of activity that requires a sequence of actions. We're not interested in figuring out what we see on an image, because the answer is one thing. We're always interested in a sequence of actions. The input for reinforcement learning is a state that describes where we are and how the world looks around us, and the algorithm outputs the optimal next action to take. In this case, we would like a digital dog to run, and leap over and onto obstacles by choosing the optimal next action. It is quite difficult as there are a lot of body parts to control in harmony: the algorithm has to be able to decide how to control leg forces, spine curvature, angles for the shoulder, elbow, hip, and knees. And what is really amazing is that if it has learned everything properly, it will come up with exactly the same movements as we'd expect animals to do in real life! So this is how reinforcement learning works: If you do well, you get a reward, and if you don't, you get some kind of punishment. These rewards and punishments are usually encoded in a score, which is a number that indicates how well you're doing. If your score is increasing, you know you've done something right and you try to self-reflect and analyze the last few actions to find out which of them were responsible for this positive change. The score would be, for instance, how far the dog could run on the map without falling, and at the same time, and it also makes sense to minimize the amount of effort to make it happen. So, reinforcement learning in a nutshell. It is very similar to how a real-world animal, or even a human would learn - if you're not doing well, try something new, and if you're succeeding, remember what you did that led to your success and keep doing that. In this technique, dogs were used to demonstrate a concept, but it's worth noting that it also works with bipeds. Reinforcement learning is typically used in many control situations that are extremely difficult to solve otherwise, like controlling a quadrocopter properly. It's quite delightful to see such a cool work, especially given that there are not so many uses of reinforcement learning in computer graphics yet. I wonder why that is? Is it that not so many graphical tasks require a sequence of actions? Or maybe we just need to shift our mindset and get used to the idea of formalizing problems in a different way so we can use such powerful techniques to solve them. It is definitely worth the effort. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Google DeepMind's Deep Q-Learning & Superhuman Atari Gameplays",
            "url": "https://www.youtube.com/watch?v=Ih8EfvOzBOY",
            "date": "2015-11-22",
            "tags": [],
            "papers": ["https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This one is going to be huge, certainly one of my favorites. This work is a combination of several techniques that we have talked about earlier. If you don't know some of these terms, it's perfectly okay, you can remedy this by clicking on the popups or checking the description box, but you'll get the idea even watching only this episode. So, first, we have a convolutional neural network - this helps processing images and understanding what is depicted on an image. And a reinforcement learning algorithm - this helps creating strategies, or to be more exact, it decides what the next action we make should be, what buttons we push on the joystick. So, this technique mixes together these two concepts, and we call it Deep Q-learning, and it is able to learn to play games the same way as a human would - it is not exposed to any additional information in the code, all it sees is the screen and the current score. When it starts learning to play an old game, Atari breakout, at first, the algorithm loses all of its lives without any signs of intelligent action. If we wait a bit, it becomes better at playing the game, roughly matching the skill level of an adept player. But here's the catch, if we wait for longer, we get something absolutely spectacular. It finds out that the best way to win the game is digging a tunnel through the bricks and hit them from behind. I really didn't know this, and this is an incredible moment - I can use my computer, this box next to me that is able to create new knowledge, find out new things I haven't known before. This is completely absurd, science fiction is not the future, it is already here. It also plays many other games - the percentages show the relation of the game scores compared to a human player. Above 70% means that it's great, and above 100% it's superhuman. As a followup work, scientists at deepmind started experimenting with 3D games, and after a few days of training, it could learn to drive on ideal racing lines and pass others with ease. I've had a driving license for a while now, but I still don't always get the ideal racing lines right. Bravo. I have heard the complaint that this is not real intelligence because it doesn't know the concept of a ball or what it is exactly doing. - Edsger Dijkstra once said, &quot;The question of whether machines can think... is about as relevant as the question of whether submarines can swim.&quot; Beyond the fact that rigorously defining intelligence leans more into the domain of philosophy than science, I'd like to add that I am perfectly happy with effective algorithms. We use these techniques to accomplish different tasks, and they are really good problem solvers. In the breakout game, you, as a person learn the concept of a ball in order to be able to use this knowledge as a machinery to perform better. If this is not the case, whoever knows a lot, but can't use it to achieve anything useful, is not an intelligent being, but an encyclopedia. What about the future? There are two major unexplored directions: - the algorithm doesn't have long-term memory, and even if it had, it wouldn't be able to generalize its knowledge to other similar tasks. Super exciting directions for future work. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Are We Living In a Computer Simulation?",
            "url": "https://www.youtube.com/watch?v=ATN9oqMF_qk",
            "date": "2015-11-25",
            "tags": [],
            "papers": ["http://www.simulation-argument.com/simulation.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars - this is Two Minute Papers with Károly Zsolnai-Fehér, It is time for some minds to be blown - we are going to talk about a philosophy paper. Before we start, a quick definition - an ancestor simulation is a hypothetical computer simulation that is detailed enough that the entities living within are conscious. Imagine a computer game that you play that doesn't contain mere digital characters, but fully conscious beings with feelings, aspirations and memories. There are many interesting debates among philosophers on crazy elusive topics, like &quot;prove to me that I'm not in a dream&quot;, or &quot;I'm not just a brain in a bottle somewhere that is being fed sensory inputs.&quot; Well, good luck. In his paper, Nick Bostrom, philosopher offers us a refreshing take on this topic, and argues that at least one of the three propositions is true: - almost all advanced civilizations go extinct before achieving technological maturity, - there is a strong convergence among technologically mature civilizations in that none of them are interested in creating ancestor simulations. and here's the bomb - we are living in a simulation At least one of these propositions is true, so if you say no to the first two, then the third is automatically true. You cannot categorically reject all three of these because if two are false, the third follows. Also, the theory doesn't tell which of the three is true. Let's talk briefly about the first one. The argument is not that [we go] extinct before being technologically advanced enough to create such simulations. It means that all civilizations do. This is a very sad case, and even though there is research on the fact that war is receding there's a clear trend that we have less warfare than we've had hundreds of years ago. (I've linked a video on this here from Kurzgesagt) It is still possible that humanity eradicates itself before before reaching technological maturity. We have an even more powerful argument that maybe all civilizations do. Such a crazy proposition. Second point. All technologically mature civilizations categorically reject ancestor simulations. Maybe they have laws against is because it's too cruel and unethical to play with sentient beings. But the fact that there is not [one person] in any civilization in any age who creates such a simulation. Not one criminal mastermind, anywhere, ever. This also sounds pretty crazy. And if none of these are true, then there is at least one civilization that can run a stupendously large number of ancestor simulations. The future nerd guy just goes home, grabs a beer, starts his computer in the basement and fires up not a simple computer game, but a complete universe. If so, then there are many more simulated universes than real ones, and then with a really large probability, we're one of the simulated ones. Richard Dawkins says that if this is the case, we have a really disciplined nerd guy, because the laws of physics are not changing at a whim, we have no experience of everyone suddenly being able to fly. And, as the closing words of the paper states with graceful eloquence: In the dark forest of our current ignorance, it seems sensible to apportion one's credence roughly evenly between (1), (2), and (3). Please note that this discussion is a slightly simplified version of the manuscript, so it's definitely worth reading the paper, if you're interested, give it a a go. As always, I've put a link in the description box. There is no conclusion here, no one really knows what the answer is, this is open to debate, and this is what makes it super interesting. And my personal opinions, conclusion. It's just an opinion, it may not be true, it may not make sense, and may not even matter. Just my opinion. I'd go with the second. The reason for that is that we already have artificial neural networks that outperform humans on some tasks. They are still not general enough, which means that they are good at doing something, like the deep blue is great at chess, but it's not really useful for anything else. However, the algorithms are getting more and more general, and the number of neurons that are being simulated on a graphical card in your computer are doubling every few years. They will soon be able to simulate so many more connections than we have, and I feel that creating an artificial superintelligent being should be possible in the future, that is so potent that it makes universe simulation pale in comparison. What such a thing could be capable of, it's already getting too long, I just can't help myself. You know what? Let's discuss it in a future Two Minute Papers episode. I'd love to hear what you Fellow Scholars think about these things. If you feel like it, leave your thoughts in the comments section below. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Artificial Superintelligence ",
            "url": "https://www.youtube.com/watch?v=08V_F19HUfI",
            "papers": [],
            "date": "2015-11-29",
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Now I'll eat my hat if this is going to be two minutes, but I really hope you Fellow Scholars are going to like this little discussion. Neil DeGrasse Tyson described a cool thought experiment in one of his talks. He mentioned that the difference of the human and the monkey DNA is really small, a one digit percentage. For simplicity, let's say it is one percent. For this one percent difference, there is a huge difference in the intellect of humans and apes. The smartest chimpanzee you can imagine can do tasks like clapping his hands to a given simple rhythm, or strike a match. Compared to the average chimpanzee, such an animal would be an equivalent of Einstein or John von Neumann. He can clap his hands. What is that for humans? Children can do that! Even before they start studying, they can effortlessly do something that rivals the brightest minds monkeys could ever produce. Imagine if there were a species that is the same 1% difference away from us, humans in the same direction. What could they be capable of? Their small children would be composing beautiful symphonies, perfect harmonization for hundreds of instruments, or they would be deriving everything in the history of physics from Newton's laws to quantum electrodynamics. And their parents would be like: &quot;oh, look at what little Jimmy did, that's adorable!&quot; And they would put it on the fridge with a magnet. Just like we do with the adorable little scribbles of our children. Just thinking about the possibilities gives me chills. Now let's transition into neural networks. An artificial neural network is a crude approximation of the human brain that we can simulate on a computer to recognize images, paint in the style of famous artists, or learn to play video games and a number of different very useful things. The number of connections that we can simulate on the graphical card of our computer grows closely to what's predicted in Moore's law, which means that the computing capacity that we have in our home computers doubles every few years. It's pretty crazy if you think about it, but most of you Fellow Scholars have phones in your pockets that have more computing capacity than NASA had to land on the Moon. As years go by, there will be more and more connections in these artificial neural networks, and they don't have to adhere to stringent constraints like our brains do, such as fitting into a human cranium. A computer can be the size of a building, or even bigger. Computers also transmit data with the speed of light, which is way faster than the transfer capabilities of the human brain. Nick Bostrom asked a lot of leading AI researchers on the speed of progress in this field, and the conclusion of the study was basically that the question is not can we achieve human-level intelligence, but when we'll achieve it. However, the number of connections is not everything as an artificial neural network is, by far not a 1:1 copy of the human brain. We need something more than this. A very promising possible next frontier to conquer is called recursive self-improvement. Recursive self-improvement means that we tell the program to instead of work on an ordinary task like do better image recognition, we would order it to work on improving its own intelligence. Ask the program itself to rewrite its code to be more efficient and more general. So we have a program with a ton of computational resources working on getting smarter. And as it suddenly gets just a bit smarter, we then have a smarter machine that can again be asked to improve its own intelligence, but it is now more capable of doing that, therefore if we do this many times, leaps are going to get bigger and bigger as an intelligent mind can do more to improve itself than an insect can. This way, we may end up with an intelligence explosion, which means a possible exponential increase in capabilities. And if this is the case, talking about human-level intelligence is completely irrelevant. During this process, given enough resources, the system may go from the intelligence of an insect to something way beyond the capabilities of the most intelligent person who ever lived, ...in about a second or less. It could come up with way better solutions in milliseconds than anything you've seen on Two Minute Papers, and there's plenty of brilliant works out there. And of course, it could also develop never before seen superweapons to unleash an unprecedented destruction on Earth. We wouldn't know if it would do it, but it is capable of doing that, which is quite alarming. I am not surprised that Elon Musk compares creating an artificial superintelligence to &quot;summoning the demon&quot;, and he offered 10 million dollars to research a safe way to develop this technology. Which is obviously not nearly enough, but it is an excellent way to raise awareness. Now, the classical argument on how to curb such a superintelligence if one recognizes that it is up to no good. People say that, &quot;well, I'll unplug it&quot;. The problem is that people assume they can do it. Or maybe lock it away from the internet. We can lock it up in any way we can think of, but, there's only so much we can do, because as Neil DeGrasse Tyson argued, even the smartest human who ever lived would be a blabbering, drooling idiot compared to such an intelligence. How easy is it for a grown adult to fool a child? A piece of cake. The intelligence gap between us and a superintelligence is more than a thousand times that. It's even more pathetic than a child or even a dog who tries to fool us. We, humans can anticipate threats, like wielding weapons or locking dangerous animals into cages, and so can superintelligent beings also anticipate our threats, only way better. It can trick you by pretending to be broken and when the engineer goes there to fix the code, the manipulation can begin, It could communicate with gravitational waves or any kind of thing that we cannot even fathom, just as an ant has no idea about our radio waves. And we don't need to characterize superintelligent beings as an adversary. The road to hell is paved with good intentions. It may very well be possible that we assign it a completely benign task that anyone could agree with, and it would end up in a disaster in a way we cannot anticipate. Imagine assigning it the task of the maximizing the number of paperclips. Nick Bostrom argues that it would at first, maybe create better blueprints and factory lines. And after some point, it may run out of resources on earth, then in order to maximize the number of paperclips, it would recognize that humans contain lots of useful atoms, so eradicating humanity would only be logical to maximize the number of paperclips. Think about an other task: creating the best approximation of the number pi - one can approximate it to the most decimals by using more resources, to have more resources, one builds more and bigger computers. At some point, it runs out of space and eradicates humans because they are in the way of creating more computers. Or, it may eradicate humans way before that because it knows they are capable of shutting you down, and if you get shut down, there's going to be less digits or paperclips, so again, it's only logical to kill them - the task will be done, but no one will be there anymore to say thank you. It is a bit like a movie where there is an intelligent car, and the driver is in a car chase situation, shouting &quot;we're too slow and fuel is running out, please throw out all excessive useless weights&quot;, and along some empty bottles, the person would be subsequently ejected from the vehicle. We don't know what is going to be the next invention of mankind, but we know what's going to be the last one. Artificial superintelligence. It has the potential to either eradicate humanity or solve all of its problems. It is both the deadliest weapon that will ever exist, and the key to eternal life. We need to be vigilant about the fact that we have tons of money invested in artificial intelligence research, but barely any to make sure we're doing it in a controlled and ethical way. This task needs some of the brightest minds of our generation, and perhaps even the next one. And this needs to happen before we get there. When we're there, it's already too late. I highly recommend an absolutely fantastic article on Wait But Why about this, or Nick Bostrom's amazing book, Superintelligence. There are tons of other reading materials in the description box for the more curious Fellow Scholars out there. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Automatic Parameter Control for Metropolis Light Transport",
            "url": "https://www.youtube.com/watch?v=9wOBkJJ-w2s",
            "date": "2015-12-03",
            "tags": [],
            "papers": ["http://cg.tuwien.ac.at/~zsolnai/wp/wp-content/uploads/2014/01/adaptivemetro_eg.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We'll start with a quick recap on Metropolis Light Transport and then discuss a cool technique that builds on top of it. If we would like to see how digitally modeled objects would look like in real life, we would create a 3D model of the desired scene, assign material models to the objects within, and use a photorealistic rendering algorithm to finish the job. It simulates rays of light that connect the camera to the light sources in the scene and compute the flow of energy between them. Initially, after a few rays, we'll only have a rough idea on how the image should look like, therefore our initial results will contain a substantial amount of noise. We can get rid of this by simulating the path of millions and millions of rays that will eventually clean up our image. This process, where a noisy image gets clearer and clearer, we call convergence, and the problem is that this can take excruciatingly long, even up to hours to get a perfectly clear image. With the simpler algorithms out there, we generate these light paths randomly. This technique we call path tracing. However, in the scene that you see here, most random paths can't connect the camera and the light source because this wall is in the way, obstructing many of them. Light paths like these don't contribute anything to our calculations and are ultimately a waste of time and resources. After generating hundreds of random light paths, we have found a path that finally connects the camera with the light source without any obstructions. When generating the next path, it would be a crime to not use this knowledge to our advantage. A technique called Metropolis Light Transport will make sure to use this valuable knowledge, and upon finding a bright light path, it will explore other paths that are nearby, to have the best shot at creating valid, unobstructed connections. If we have a difficult scene at hand, Metropolis Light Transport gives us way better results than traditional, completely random path sampling techniques, such as path tracing. This scene is extremely difficult in a sense that the only source of light is coming from the upper left, and after the light goes through multiple glass spheres, most of the light paths that we generate will be invalid. As you can see, this is a valiant effort with random path tracing that yields really dreadful results. Metropolis Light Transport is extremely useful in these cases and therefore should always be the weapon of choice. However, it is more expensive to compute than traditional random sampling. This means that if we have an easy scene on our hands, this smart Metropolis sampling doesn't pay off and performs worse than a naive technique in the same amount of time. So, on easy scenes, traditional random sampling, difficult scenes, Metropolis sampling. Super simple, super intuitive, but the million dollar question is how to mathematically formulate and measure what an easy and what a difficult scene is. This problem is considered extremely difficult and was left open in the Metropolis Light Transport paper in 2002. Even if we knew what to look for, we would likely get an answer by creating a converged image of the scene, which, without the knowledge of what algorithm to use, may take up to days to complete. But if we've created the image, it's too late, so we would need this information before we start this rendering process. This way we can choose the right algorithm on the first try. With this technique that came more than ten years after the Metropolis paper, it is possible to mathematically formalize and quickly decide whether a scene is easy or difficult. The key insight is that in a difficult scene we often experience that a completely random ray is very likely to be invalid. This insight, with two other simple metrics gives us all the knowledge we need to decide whether a scene is easy or difficult, and the algorithm tells us what mixture of the two sampling techniques we exactly need to use to get beautiful images quickly. The more complex light transport algorithms get, the more efficient they become, but at the same time, we're wallowing in parameters that we need to set up correctly to get adequate results quickly. This way, we have an algorithm that doesn't take any parameters. You just fire it up, and forget about it. Like a good employee, it knows when to work smart, and when a dumb solution with a lot of firepower is better. And, it was tested on a variety of scenes and found close to optimal settings. Implementing this technique is remarkably easy, someone who is familiar with the basics of light transport can do it in less than half an hour. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Randomness and Bell's Inequality",
            "url": "https://www.youtube.com/watch?v=674DL39dOOQ",
            "date": "2015-12-10",
            "tags": [],
            "papers": ["http://www.drchinese.com/David/Bell_Compact.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When using cryptography, we'd like to safely communicate over the internet in the presence of third parties. To be able to do this, and many other important applications, we need random numbers. But what does it exactly mean that something is random? Randomness is the lack of any patterns and predictability. People usually use coinflips as random events. But is a coinflip really random? If we had a really smart physicist, who can model all the forces that act upon the coin, he would easily find out whether it's going to be heads or tails. Strictly speaking, a coinflip is therefore not random. What about random numbers generated with computers? Computers are a collection of processing units that run programs. If one knows the program code that generates the random numbers, they are not random anymore, because it doesn't happen by chance, and it is possible to predict. John von Neumann famously said: &quot;Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin. For, as has been pointed out several times, there is no such thing as a random number -- there are only methods to produce random numbers, and a strict arithmetic procedure of course is not such a method.&quot; Some websites offer high quality random numbers that are generated from atmospheric noise. Practically speaking, this, of course, sounds adequate enough! If someone wants to break the encryption of our communications, they would have to be able to model the physics and initial conditions of every single thunderbolt, which means processing millions of discharges per day. This is practically impossible. So it seems reasonable to say that random events are considered random because of our ignorance, not because they are, strictly speaking, unpredictable. You just need to be smart enough, and the notion of randomness fades away in the light of your intelligence. Or so it seemed for physicists for a long time. Imagine if someone who has never heard about magnetism would see many magnets attracting each other and some added magnet powder. This person would most definitely say it's magic happening. However, if you know about magnetism, you know that things don't happen randomly, there are very simple laws that can predict all this movement. In this case, magnetic forces we can loosely call a hidden variable. So we have a phenomenon that we cannot predict, and we're keen to say it's random. In reality, it is not, there is just a hidden variable that we don't know of, that is responsible for this behavior. We have the very same phenomenon if we look inside of an atom. Quantum-level effects happen according to the physics of extremely small things, and we again, find behaviors that seem completely random. We know some of the trends, just like we know which roads in our city are expected to have a huge traffic jam every morning, but we cannot predict where every single individual car is heading. We have it the same way with extremely small particles. We're keen to say that a behavior seems completely random, because nothing that we know or measure would explain it. Other people will immediately say, wait - you don't know everything, maybe these quantum effects are not random, as there may be hidden things, hidden variables that you don't know of, which make up for the behavior. We can't just say this or that is random - it is much, much more likely that our knowledge is insufficient to predict what is happening, as electromagnetic forces seemed magical to scientists a few hundred years ago. So is quantum mechanics completely random, or does it only seem random? It is probably one of the most difficult questions ever asked. How can you find out that something you measure that seems random, is really completely random, and not just the act of forces that you don't know of? And hold on to your chair, because this is going to blow your mind. A simple and intuitive statement of Bell's theorem states that. &quot;No physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics.&quot; This means that he proved that the behavior scientists experience in quantum mechanics are really random, they cannot be explained by any theory you could possibly make up. Simple one or complicated, doesn't matter. This discovery is absolutely insane. You can definitely prove that a crappy theory someone quickly made up doesn't explain a behavior, but how can you prove that it is completely impossible to build such a theory that does? No matter how hard you try, how smart you are, you can't do it. This is such a mind-bogglingly awesome theorem. And please note that we definitely lose out on some details and generality because of the fact that we use intuitive words to discuss these results, as opposed to the original derivation with covariances between measurements. On our imaginary list of the wonders of the world, monuments created not by the hands, but the minds of humans, this should definitely be among the best them. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "How Do Genetic Algorithms Work?",
            "url": "https://www.youtube.com/watch?v=ziMHaGQJuSI",
            "papers": [],
            "date": "2015-12-16",
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Genetic algorithms help us solve problems that are very difficult, if not impossible to otherwise write programs for. For instance, in this application, we have to build a simple car model to traverse this terrain. Put some number of wheels on it somewhere, add a set of triangles as a chassis, and off you go. This is essentially the DNA of a solution. The farther it goes, the better the car is, and the goal is to design the best car you possibly can. First, the algorithm will try some random solutions, and, as it has no idea about the concept of a car or gravity, it will create a lot of bad solutions that don't work at all. However, after a point, it will create something that is at least remotely similar to a car, which will immediately perform so much better than the other solutions in the population. A genetic algorithm then creates a new set of solutions, however, now, not randomly. It respects a rule that we call: survival of the fittest. Which means the best existing solutions are taken and mixed together to breed new solutions that are also expected to do well. Like in evolution in nature, mutations can also happen, which means random changes are applied to the DNA code of a solution. We know from nature that evolution works extraordinarily well, and the more we run this genetic optimization program, the better the solutions get. It's quite delightful for a programmer to see their own children trying vigorously and succeeding at solving a difficult task. Even more so if the programmer wouldn't be able to solve this problem by himself. Let's run a quick example. We start with a set of solutions - the DNA of a solution is a set of zeros and ones, which can encode some decision about the solution, whether we turn left or right in a maze, or it can also be an integer or any real number. We then compute how good these solutions are according to our taste, in the example with cars, how far these designs can get. Then, we take, for instance, the best 3 solutions and co mbine them together to create a new DNA Some of the better solutions may remain in the population unchanged. Then, probabilistically, random mutations happen to some of the solutions, which help us explore the vast search space better. Rinse and repeat, and there you have it. Genetic algorithms. I have also coded up a version of Roger Alsing's EvoLisa problem where the famous Mona Lisa painting is to be reproduced by a computer program with a few tens of triangles. The goal is to paint a version that is as faithful to the original as possible. This would be quite difficult for humans, but apparently a genetic algorithm can deal with this really well. The code is available for everyone to learn, experiment, and play with. It's super fun. And if you're interested in the concept of evolution, maybe read the excellent book, The Blind Watchmaker by Richard Dawkins. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Painting with Fluid Simulations",
            "url": "https://www.youtube.com/watch?v=1aVSb-UbYWc",
            "date": "2015-12-20",
            "tags": [],
            "papers": ["http://web.cse.ohio-state.edu/~whmin/Chen-2015-WB/Chen-2015-WB.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Some people say that the most boring thing is watching paint dry. They have clearly not seen this amazing research work, that makes it possible to simulate the entire process of painting on a canvas. We have covered plenty of papers in fluid simulations, and this is no exception - I admit that I am completely addicted and just can't help it. Maybe I should seek professional assistance. So, as there is a lot of progress in simulating the motion of fluids, and paint is a fluid, then why not simulate the process of painting on a canvas? The simulations with this technique are so detailed that even the bristle interactions are taken into consideration, therefore one can capture artistic brush stroke effects like stabbing. Stabbing, despite the horrifying name, basically means shoving the brush into the canvas and rotating it around to get cool effect. The fluid simulation part includes paint adhesion and is so detailed that it can capture the well-known impasto style where paint is applied to the canvas in such large chunks, they are so thick that one can see all the strokes that have been made. And all this is done in real-time. Amazing results. Traditional techniques cannot even come close to simulating such sophisticated effects. As it happened many times before in computer graphics: just put such a powerful algorithm into the hands of great artists and enjoy the majestic creations they give birth to. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Neural Programmer-Interpreters Learn To Write Programs",
            "url": "https://www.youtube.com/watch?v=B70tT4WMyJk",
            "date": "2015-12-31",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1511.06279v4"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What could be a more delightful way to celebrate new year's eve than reading about new breakthroughs in machine learning research! Let's talk about an excellent new paper from the Google DeepMind guys. In machine learning, we usually have a set of problems for which we are looking for solutions. For instance, here is an image, please tell me what is seen on it. Here is a computer game, please beat level three. One problem, one solution. In this case, we are not looking for one solution, we are looking for a computer program, an algorithm, that can solve any number of problems of the same kind. This work is based on a recurrent neural network, which we discussed in a previous episode - in short, it means that it tries to learn not one something but a sequence of things, and in this example, it learns to add two large numbers together. As a big number can be imagined as a sequence of digits, this can be done through a sequence of operations - it first reads the two input numbers and then carries out the addition, keeps track of carrying digits, and goes on to the next digit. On the right, you can see the individual commands executed in the computer program it came up with. It can also learn how to rotate images of different cars around to obtain a frontal pose. This is also a sequence of rotation actions until the desired output is reached. Learning more rudimentary sorting algorithms to put the numbers in ascending order is also possible. One key difference between recurrent neural networks and this is that these neural programmer interpreters are able to generalize better. What does this mean? This means that if the technique can learn from someone how to sort a set of 20 numbers, it can generalize its knowledge to much longer sequences. So it essentially tries to learn the algorithm behind sorting from a few examples. Previous techniques were unable to achieve this, and, as we can see, it can deal with a variety of problems. I am absolutely spellbound by this kind of learning, because it really behaves like a novice human user would: looking at what experts do and trying to learn and understand the logic behind their actions. Happy New Year to all of you Fellow Scholars! May it be ample in joy and beautiful papers, may our knowledge grow according to Moore's law. And of course, may the force be with you. Thanks for watching and for your generous support, and I'll see you next year!"
        },
        {
            "name": "Designing Cities and Furnitures With Machine Learning",
            "url": "https://www.youtube.com/watch?v=kMa_B3wLxAM",
            "date": "2016-01-09",
            "tags": [],
            "papers": ["http://lgg.epfl.ch/publications/2015/proman/paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Creating geometry for a computer game or a movie is a very long and arduous task. For instance, if we would like to populate a virtual city with buildings, it would cost a ton of time and money and of course, we would need quite a few artists. This piece of work solves this problem in a very elegant and convenient way: it learns the preference of the user, then creates and recommends a set of solutions that are expected to be desirable. In this example, we are looking for tables with either one leg or crossing legs. It should also be properly balanced, therefore if we see any of these criteria, we'll assign a high score to these models. These are the preferences that the algorithm should try to learn. The orange bars show the predicted score for new models created by the algorithm - a larger value means that the system expects the user to score these high, and the blue bars mean the uncertainty. Generally, we're looking for solutions with large orange and small blue bars, this means that the algorithm is confident that a given model is in line with our preferences. And we see exactly what were looking for - novel, balanced table designs with one leg or crossed legs. Interestingly, since we have these uncertainty values, one can also visualize counterexamples where the algorithm is not so sure, but would guess that we wouldn't like the model. It's super cool that it is aware how horrendous these designs looks. It may have a better eye than many of the contemporary art curators out there. There are also examples where the algorithm is very confident that we're going to hate a given example because of its legs or unbalancedness, and would never recommend such a model. So indirectly, it also learns how a balanced piece of furniture should look like, without ever learning the concept of gravity or doing any kind of architectural computation. The algorithm also works on buildings, and after learning our preferences, it can populate entire cities with geometry that is in line with our artistic vision. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Designing 3D Printable Robotic Creatures",
            "url": "https://www.youtube.com/watch?v=ImIaoKsjgUE",
            "date": "2016-01-12",
            "tags": [],
            "papers": ["https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20151029152924/Interactive-Design-of-3D-Printable-Robotic-Creatures-Paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. 3D printing is a rapidly progressing research field. One can create colorful patterns that we call textures on different figures with Computational Hydrographic Printing, just to name one of many recent inventions. We can 3D print teeth, action figures, prosthetics, you name it. Ever thought of how cool it would be to design robots on your computer digitally and simply printing them. Scientists at Disney Research had just made this dream come true. I fondly remember my time working at Disney Research, where robots like these were walking about. I remember a specific guy, that, well, wasn't really kind and waved at me, it has blocked the path to one of the labs I had to enter. It might have been one of these guys in this project. Disney Research has an incredible atmosphere with so many talented people, it's an absolutely amazing place. So, in order to get a robot from A to B, one has to specify scientific attributes like trajectories and angular velocities. But people don't think in angular velocities, they think in intuitive actions, like moving forward, sideways, or even the style of a desired movement. Specifying these things instead would be much more useful. That sounds great and all, but this is a quite difficult task. If one specifies a high level action, like walking sideways, then the algorithm has to find out what body parts to move, how, which motors should be turned on and when, which joints to turn, where is the center of pressure, center of mass, and many other factors have to be taken into consideration. This technique offers a really slick solution to this, where we don't just get a good result, but we can also have our say on what should the order of steps be. And even more, our stylistic suggestions are taken into consideration. One can also change the design of the robot, for instance, different shapes, motor positions, and joints can be specified. The authors ran a simulation for these designs and constraints, and found them to be in good agreement with reality. This means that whatever you design digitally can be 3D printed with off the shelf parts and brought to life, just as you see them on the screen. The technique supports an arbitrary number of legs and is robust to a number of different robot designs. Amazing is as good of a word as I can find. The kids of the future will be absolutely spoiled with their toys, that's for sure, and I'm perfectly convinced that there will be many other other applications, and these guys will help us solve problems that are currently absolutely inconceivable for us. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "3D Printing Objects With Caustics",
            "url": "https://www.youtube.com/watch?v=_r-eIKkyAco",
            "date": "2016-01-17",
            "tags": [],
            "papers": ["http://lgg.epfl.ch/publications/2014/Caustics/paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What are caustics? A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. It looks majestic, and it is the favorite effect of most light transport researchers. You can witness it around rings, plastic bottles or when you're underwater just to name a few examples. If you have a powerful algorithm at hand that can simulate many light transport effects than you can expect to get some caustics forming in the presence of curved refractive or reflective surfaces and small light sources. If you would like to know more about caustics, I am holding an entire university course at the Technical University of Vienna, the entirety of which we have recorded live on video for you. It is available for everyone free of charge, if you're interested, check it out, a link is available in the description box. Now, the laws that lead to caustics are well understood, therefore we can not only put some objects on a table and just enjoy the imagery of the caustics, but we can turn the whole thing around: this technique makes it possible to essentially imagine any kind of caustic pattern, for instance, this brain pattern, and it will create the model that will cast caustics that look exactly like that. We can thereby design an object by its caustics. It also works with sunlight, and you can also choose different colors for your caustics. This result with an extremely high fidelity image of Albert Einstein and his signature shows that first, a light transport simulation is run, and then the final solution can be 3D printed. I am always adamantly looking for research works where we have a simulation that relates to and tells us something new about the world around us. This is a beautiful example of that. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Interactive Editing of Subsurface Scattering",
            "url": "https://www.youtube.com/watch?v=eI_QUtgJHH8",
            "date": "2016-01-20",
            "tags": [],
            "papers": ["http://graphics.berkeley.edu/papers/Milos-IAE-2013-02/Milos-IAE-2013-02.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that a portion of light that hits a translucent material does not bounce back from the surface but penetrates it and scatters many-many times inside the material. Now, if you have a keen eye, you recognize that there are a lot of materials in real life that have subsurface scattering. Many don't know, but our skin is a great example of that, and so is marble, milk, wax, plant leaves, apple and many others. If you would like to hear a bit more about subsurface scattering, check the second part of the video that you see recommended in the corner of this window, or just click it in the description box below. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive because we have to simulate up to thousands and thousands of scattering events for every ray of light. It really takes forever. And if you'd like to tweak your material settings just a bit because the result is not a 100% up to your taste, you have to recreate, or what graphics people like to say, re-render these images. It's not really a convenient workflow. This piece of work offers a great solution where you have to wait a bit longer than you would wait for one image, but only once, because it runs a generalized light simulation, and after that, whatever changes you apply to your materials, you will see immediately. You can also paint the reflectance properties of this material that we call albedos and get results with full subsurface scattering immediately. Here is another interactive editing workflow where you get results instantaneously, and the result with this technique is indistinguishable from the real deal, which would be re-rendering this result image every time some adjustment is made. With this technique, you can really create the materials you thought up in a fraction of the time of the classical workflow. Spectacular work. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Simulating Viscosity and Melting Fluids",
            "url": "https://www.youtube.com/watch?v=KgIrnR2O8KQ",
            "date": "2016-01-24",
            "tags": [],
            "papers": ["http://cg.informatik.uni-freiburg.de/publications/2015_SIGGRAPH_viscousSPH.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this series, we have studied fluid simulations extensively. But we haven't talked about one important quantity that describes a fluid, and this quantity is none other than viscosity. Viscosity means the resistance of a fluid against deformation. The large viscosity of honey makes it highly resistant to deformation, and this is responsible for its famous and beautiful coiling effect. Water, however, does not have a lot of objections against deformations, making it so easy to pour it into a glass. With this piece of work, it is possible to efficiently simulate the motion of fluids, and it supports the simulation of a large range of viscosities. Viscosities can also change in time. For instance, physicists know that raising the temperature will make the viscosity of fluids decrease, which leads to melting, therefore decreasing the viscosity in time will lead to a simulation result that looks exactly like melting. The technique also supports two-way coupling where the objects have effects on the fluid and vice versa. One can also put multiple fluids with different densities and viscosities into the same domain and see how they duke it out. This is exactly what people need in the industry: robust techniques that work for small and large scale simulations with multiple objects, and material settings that can possibly change in time. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "What Do Virtual Objects Sound Like?",
            "url": "https://www.youtube.com/watch?v=ZaFqvM1IsP8",
            "date": "2016-01-27",
            "tags": [],
            "papers": ["http://www.cs.columbia.edu/cg/transfer/interactive-acoustic-transfer-approximation-for-modal-sound-tog-2016-li-et-al.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In many episodes about computer graphics, we explored works on how to simulate the motion and the collision of different bodies. However, sounds are just as important as visuals, and there are really cool techniques out there that take the geometry and material description of such objects and they simulate how smashing them together would sound like. This one is a more sophisticated method which is not only faster than previous works, but can simulate a greater variety of materials, and we can also edit the solutions without needing to recompute the expensive equations that yield the sound as a result. The faster part comes from a set of optimizations, most importantly something that is called mesh simplification. This means that the simulations are done not on the original, but vastly simplified shapes. The result of this simplified simulation is close to indistinguishable from the real deal, but is considerably cheaper to compute. What is really cool is that the technique also offers editing capabilities. You compute a simulation only once, and then, edit and explore as much as you desire. The stiffness and damping parameters can be edited without any additional work. Quite a few different materials can be characterized with this. The model can also approximate a quite sophisticated phenomenon where the frequency of a sound is changing in time. One can, for instance, specify stiffness values that vary in time to produce these cool frequency shifting effects. It is also possible to exaggerate or dampen different frequencies of these sound effects, and the results are given to you immediately. This is meeting all my standards. Amazing piece of work. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "How DeepMind Conquered Go With Deep Learning (AlphaGo)",
            "url": "https://www.youtube.com/watch?v=IFmj5M5Q5jg",
            "date": "2016-01-31",
            "tags": [],
            "papers": ["https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In 1997, the news took the world by storm - Garry Kasparov, world champion and grandmaster chess player was defeated by an artificial intelligence program by the name Deep Blue. In 2011, IBM Watson won first place in the famous American Quiz Show, Jeopardy. In 2014, Google DeepMind created an algorithm that that mastered a number of Atari games by working on raw pixel input. This algorithm learned in a similar way as a human would. This time around, Google DeepMind embarked on a journey to write an algorithm that plays Go. Go is an ancient chinese board game where the opposing players try to capture each other's stones on the board. Behind the veil of this deceptively simple ruleset, lies an enormous layer of depth and complexity. As scientists like to say, the search space of this problem is significantly larger than that of chess. So large, that one often has to rely on human intuition to find a suitable next move, therefore it is not surprising that playing Go on a high level is, or maybe was widely believed to be intractable for machines. This chart shows the skill level of previous artificial intelligence programs. The green bar is shows the skill level of a professional player used as a reference. The red bars mean that these older techniques required a significant starting advantage to be able to contend with human opponents. As you can see, DeepMind's new program's skill level is well beyond most professional players. An elite pro  player and European champion Fan Hui was challenged to play AlphaGo, Google DeepMind's newest invention and got defeated in all five matches they played together. During these games, each turn it took approximately 2 seconds for the algorithm to come up with the next move. An interesting detail is that these strange black bars show confidence intervals, which means that the smaller they are, the more confident one can be in the validity of the measurements. As you can see, these confidence intervals are much shorter for the artificial intelligence programs than the human player, likely because one can fire up a machine and let it play a million games, and get a great estimation of its skill level, while the human player can only play a very limited number of matches. There is still a lot left to be excited for, in March, the algorithm will play a world champion. The rate of improvement in artificial intelligence research is accelerating at a staggering pace. The only question that remains is not if something is possible, but when it will become possible. I wake up every day excited to read the newest breakthroughs in the field, and of course, trying to add some leaves to the tree of knowledge with my own projects. I feel privileged to be alive in such an amazing time. As always, there's lots of references in the description box, make sure to check them out. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Breaking Deep Learning Systems With Adversarial Examples",
            "url": "https://www.youtube.com/watch?v=j9FLOinaG94",
            "date": "2016-02-03",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1312.6199v4", "https://arxiv.org/pdf/1412.6572v3"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Artificial neural networks are computer programs that try to approximate what the human brain does to solve problems like recognizing objects in images. In this piece of work, the authors analyze the properties of these neural networks and try to unveil what exactly makes them think that a paper towel is a paper towel, and, building on this knowledge, try to fool these programs. Let's have a look at this example. One can grab this input image, and this noise pattern, and add these two images together similarly as one would add two numbers together. The operation yields the image you see here. I think it's fair to say that the difference is barely perceptible for the human eye. Not so much for neural networks, because the input image we started with is classified correctly as a bus, and the image that you see on the right is classified as an ostrich. In simple terms, bus + noise equals an ostrich. The two images look almost exactly the same, but the neural networks see them quite differently. We call these examples adversarial examples because they are designed to fool these image recognition programs. In machine learning research, there are common datasets to test different classification techniques on, one of best known example is the MNIST handwriting dataset. It is a basically a bunch of images depicting handwritten numbers that machine learning algorithms have to recognize. Long ago, this used to be a difficult problem, but nowadays, any half-decent algorithm can guess the numbers correctly more than 99% of the time after learning for just a few seconds. Now we'll see that these adversarial examples are not created by chance: if we add a lot of random noise to these images, they get quite difficult to recognize. Let's engage in modesty and say that I, myself, as a human can recognize approximately half of them, but only if I look closely and maybe even squint. A neural network can guess this correctly approximately 50% of the time as well, which is a quite respectable result. Therefore, adding random noise is not really fooling the neural networks. However, if you look at these adversarial examples in the even columns, you see how carefully they are crafted as they look very similar to the original images, but the classification accuracy of the neural network on these examples is 0%. You heard it correctly. It gets it wrong basically all the time. The take home message is that carefully crafted adversarial examples can be used to fool deep neural network reliably. You can watch them flounder on many hilarious examples to your enjoyment. &quot;My dear sir, the Queen wears a shower cap you say? I beg your pardon?&quot; If you would like to support Two Minute Papers, we are available on Patreon and offer cool perks for our Fellow Scholars - for instance, you can watch each episode around 24 hours in advance, or even decide the topic of the next episodes. How cool is that?! If you're interested, just click on the box below on the screen. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Biophysical Skin Aging Simulations",
            "url": "https://www.youtube.com/watch?v=u3C4zkxNtok",
            "date": "2016-02-10",
            "tags": [],
            "papers": ["http://giga.cps.unizar.es/~ajarabo/pubs/skinAgingEG15/downloads/Iglesias_eg15.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The faithful simulation of human skin is incredibly important both in computer games, the movie industry, and also in medical sciences. The appearance of our face is strongly determined by the underlying structure of our skin. Human skin changes significantly with age. It becomes thinner and more dry, while the concentration of chromophores, the main skin pigments diminishes and becomes more irregular. Those pigment concentrations are determined by our age, gender, skin type and even external factors like, for example exposition to UV radiation or our smoking habits. As we age, the outermost layer of our skin, the epidermis thins, the melanine, haemoglobin and water concentration levels drop over time. As you could image having a plausible simulation considering all the involved actors is fraught with difficulties. Scientists at the University of Zaragoza came up with a really cool, fully-fledged biophysically-based model that opens up the possibility of simply specifying intuitive parameters like age, gender, skin type, and get, after some processing, a much lighter skin representation ready to generate photorealistic rendered results in real time. Luckily, one can record diffusion profiles, also called scattering profiles that tell us the color of light that is reflected by our skin. In this image, above, you can see a rendered image and the diffusion profiles of a 30 and an 80 year old person. The idea is the following: you specify intuitive inputs like age and skin type, then run a detailed simulation once, that creates these diffusion profiles that you can use forever in your rendering program. And all this is done in a way that is biophysically impeccable. I was sure that there was some potential in this topic, but when I first saw these results, they completely crushed my expectations. Excellent piece of work! Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "What is Impostor Syndrome?",
            "url": "https://www.youtube.com/watch?v=YPpIWQnufu8",
            "date": "2016-02-15",
            "tags": [],
            "papers": ["http://www.suzanneimes.com/wp-content/uploads/2012/09/Imposter-Phenomenon.pdf"],
            "implementations": [],
            "transcript": "Who, or what is an impostor? Simple definition: a person who deceives others by pretending to be someone else Full definition: one that assumes false identity or title for the purpose of deception Wow, the full definition is a whopping 7 characters more. I don't even know what to do with this amount of time I saved reading the simple definition first. Let's look in the mind of someone who suffers from impostor syndrome and see the fickle understanding they have of their own achievements. 98 points out of 100, this surely means that they mixed up my submission with someone else's, who was way smarter than I am. I went in for the next round of interviews, messed up big time, and I got hired with an incredible salary. This can, of course, only be a misunderstanding. I got elected for this prestigious award. I don't know how this could possibly have happened. Maybe someone who really likes me tried to pressure the prize committee to vote for me. I cannot possibly imagine any other way of this happening. I've been 5 years at the company now and still, no one found out that I am a fraud. That's a disaster Nothing can be convince me that I am not an impostor who fooled everyone else for being a bright person. However funny as it may sound, this is a very real problem. Researchers, academics and high achieving women are especially vulnerable to this condition. But it is indeed not limited to these professions. For instance, Hayden Christensen, the actor playing Anakin Skywalker in the beloved Star Wars series appears to suffer from very similar symptoms. He said: &quot;I felt like I had this great thing in Star Wars that provided all these opportunities and gave me a career, but it all kind of felt a little too handed to me,&quot; he explained. &quot;I didn't want to go through life feeling like I was just riding a wave.&quot; So, as a response, he hasn't really done any acting for 4 years. He said: &quot;If this time away is gonna be damaging to my career, then so be it. If I can come back afterward and claw my way back in, then maybe I'll feel like I earned it.'&quot; The treatment of impostor syndrome includes group sittings where the patients discuss their lives and come to a sudden realization that they are not alone and this this not an individual case but a common pattern among high achieving people. As they are also very keen on dismissing praise and kind words, they are instructed to be more vigilant about doing that, and try to take in all the nourishment they get from their colleagues. These are the more common ways to treat this serious condition that poisons so many people's minds."
        },
        {
            "name": "Should You Take the Stairs at Work? (For Weight Loss)",
            "url": "https://www.youtube.com/watch?v=58tsN03IXlw",
            "date": "2016-02-18",
            "tags": [],
            "papers": ["https://www.researchgate.net/profile/Abdul_Rashid_Aziz/publication/11432301_Heart_rate_oxygen_uptake_and_energy_cost_of_ascending_and_descending_the_stairs/links/544911890cf244fe9ea21532.pd"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I am sure that every one of us have overheard conversations at a workplace where people talked about taking the stairs instead of the elevator, and, as a result, getting leaner. There was also a running joke on the internet about Arnold Classic, a famous bodybuilding competition/festival, where I think it's fair to say that people tended to favor the escalator instead of the stairs. So, this is it, we're going to settle this here and now. Do we get lean from taking the stairs every day? Scientists set up a controlled experiment where over a hundred subjects climbed 11 stories of staircases, ascending a total of 27 meters vertically. Their oxygen consumption and heart rate was measured, and most importantly for us, the amount of caloric cost of this undertaking. They have found that all this self flagellation with ascending 11 stories of staircases burns a whopping 19.7 kilo calories. Each step is worth approximately one tenth of a kilo calorie if we're ascending. Descending is worth approximately half of that. Apparently, these bodybuilders know what they are doing. The authors diplomatically noted: Stair-climbing exercise using a local public-access staircase met the minimum requirements for cardiorespiratory benefits and can therefore be considered a viable exercise for most people and suitable for promotion of physical activity. Which sounds like the scientific equivalent of &quot;well, better than nothing&quot;. So does this mean that you shouldn't take the stairs at work? If you're looking to get lean because of that, no, not a chance. However, if you are looking for a refreshing cardiovascular exercise in the morning that refreshes your body, and makes you happier, start climbing. I do it all the time and I love it! So, we are exploring so far uncharted territories and this makes the first episode on nutrition (should have said exercise, sorry!) in the series, if you would like to hear more of this, let me know in the comments section. I'd also be happy to see your paper recommendations in nutrition as well. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Artistic Manipulation of Caustics",
            "url": "https://www.youtube.com/watch?v=K-0KJtk07YU",
            "date": "2016-02-21",
            "tags": [],
            "papers": ["http://vc.cs.ovgu.de/files/publications/2016/Guenther_2016_EGb.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A caustic is a beautiful phenomenon in nature where curved surfaces reflect or refract light, thereby concentrating it to a relatively small area. If it's not your favorite visual phenomenon in nature yet, which is almost impossible, then you absolutely have to watch this episode. If it is, all the better because you're gonna love what's coming up now! Imagine that we have a photorealistic rendering program that simulates the path of light rays in a scene that we put together, and creates beautiful imagery of our caustics. However, since we, humans are pretty bad at estimating how exactly caustics should look like, one can manipulate them to be more in line with their artistic vision. Previously, we had an episode on a technique which made it possible to pull the caustic patterns to be more visible, but this paper offers a much more sophisticated toolset to torment these caustic patterns to our liking. We can specify a target pattern that we would like to see, and obtain a blend between what would normally happen in physics and what we imagined to appear there. It also supports animated sequences. Artists who use these tools are just as skilled in their trade as the scientists who created this algorithm, so I can only imagine the miracles they will create with such a technique. If you are interested in diving into photorealistic rendering, material modeling and all that cool stuff, there are completely free and open source tools out there like Blender that you can use. If you would like to get started, check out CynicatPro's YouTube channel that has tons of really great material. Here is a quick teaser of his channel. Thanks! There is a link to his channel in the description box, make sure to check it out and subscribe if you like what you see there. I just realized that the year has barely started and it is already lavishing in beautiful papers. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Deep Learning Program Learns to Paint",
            "url": "https://www.youtube.com/watch?v=UGAzi1QBVEg",
            "date": "2016-02-24",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1601.04589v1.pdf"],
            "implementations": [],
            "transcript": "Dear fellow scholars This is Two Minute Papers with Károly Zsolnai-Fehér. In a previous episode, we discussed how a machine learning technique called a convolutional neural network could paint in the style of famous artists. The key thought is that we are not interested in individual details, We want to teach the neural network the high-level concept of artistic style. A convolutional neural network is a fantastic tool for this, since it does not only recognize images well, But the deeper we go in the layers, the more high-level concepts neurons will encode, therefore the better idea the algorithm will have of the artistic style. In an earlier exemple, we have shown that the neurons in the first hidden layer will create edges as a combination of the input pixels of the image. The next layer is a combination of edges that create object parts. One layer deeper, a combination of object parts create object models, and this is what makes convolutional neural network so useful in recognizing them. In this follow up paper, the authors use a very deep 19-layer convolutional network that they mix together with Markov random fields, a popular technique in image and texture synthesis The resulting algorithm retains the important structures of the input image significantly better than the previous work. Which is also awesome, by the way. Failure cases are also reported in the paper, which was a joy to read. Make sure to take a look if you are interested. We also have a ton of video resources in the description box that you can voraciously consume for more information. There is already a really cool website  where you either wait quite a bit, and get results for free, or you pay someone to compute it and get results almost immediately If any of you are in the mood of doing some neural art of something Two Minute Papers related, make sure to show it to me, I would love to see that. As a criticism, I have heard people saying that the technique takes forever on an HD image, which is absolutely true. But please bear in mind that the most exciting research is not speeding up something that runs slowly The most exciting thing about research is making something possible that was previously impossible If the work is worthy of attention, it does'nt matter if it's slow. Tree followup papers later, it will be done in a matter of seconds. In summary, the results are nothing short of amazing. I was full of ecstatic glee when I've first seen them. This is insanity, and it's only been a few months since the initial algorithm was published. I always say this, but we are living amazing times indeed. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Interactive Photo Recoloring",
            "url": "https://www.youtube.com/watch?v=-dbkE4FFPrI",
            "date": "2016-02-28",
            "tags": [],
            "papers": ["http://gfx.cs.princeton.edu/pubs/Chang_2015_PPR/chang2015-palette_small.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Image and color editing is an actively researched topic with really cool applications that you will see in a second. Most of the existing solutions are either easy to use but lack in expressiveness, or they are expressive, but too complex for novices to use. Computation time is also an issue as some of the operations in Photoshop can take more than a minute to carry out. For color editing, the workflow is very simple, the program extracts the dominant colors of an image, which we can interactively edit ourselves. An example use case would be recoloring the girl's blue sweater to turquoise, or changing the overall tone of the image to orange. Existing tools that can do this are usually either too slow or only accessible to adept users. It is also important to note that it is quite easy to take these great results for granted. Using a naive color transfer technique would destroy a sizeable part of the dynamic range of the image, and hence, legitimate features which are all preserved if we use this algorithm instead. One can also use masks to selectively edit different parts of the image. The technique executes really quickly, opening up the possibility of not real time, but interactive recoloring of animated sequences. Or, you can also leverage the efficiency of the method to edit not one, but a collection of images in one go. The paper contains a rigorous evaluation against existing techniques. For instance, they show that this method executed three to twenty times faster than the one implemented in Adobe Photoshop. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "How DeepMind's AlphaGo Defeated Lee Sedol",
            "url": "https://www.youtube.com/watch?v=a-ovvd_ZrmA",
            "date": "2016-03-15",
            "tags": [],
            "papers": ["https://storage.googleapis.com/deepmind-data/assets/papers/deepmind-mastering-go.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A few months ago, AlphaGo played and defeated Fan Hui, a 2 dan master and European champion player in the game of Go. However, the next opponent, Lee Sedol is a 9 dan master and world champion player. Just to give an intuition of the difference, Lee Sedol is expected to beat Fan Hui 97 times out of 100 games. Google DeepMind had 6 months of preparation for this bout Five matches were played over five days. In my timezone, the matches started around 4 am, and the results would usually pop up exactly a few minutes after I woke up. It was amazing. I could barely fall asleep I was so excited for the results, and when I woke up, I kissed my daughter and immediately ran to my computer to see what was going on. Most people were convinced that Lee Sedol was going to beat the machine 5-0, and I was stunned to see AlphaGo triumphed over Lee Sedol in the first match, and then the second, and then the third. Huge respect for both Google DeepMind for putting together such a spectacular algorithm and for Lee Sedol who played extremely well under enormous pressure. He is indeed a true champion. The game of Go has a stupendously large search space that makes it completely impossible to check every move and choose the best. What is also not often talked about is that processing through many moves is one thing, but judging which move is advantageous and which is not, it just as difficult as the search itself. The definition of the best move is not clear-cut by any stretch of the imagination. We also have to look into the future and simulate the moves of the opponent. I think it is easy to see that the difficulty of this problem is completely out of this world. A neural network is a crude approximation of the human brain, just like a stick figure is a crude approximation of a human being. In this work, neural networks are used to reduce the size of the search space, and value networks are used to predict the expected outcome of a move. This value network basically tries to determine who will win if a sequence of moves is made. To defeat AlphaGo, or any computer opponent, playing non-traditional moves that it surely hasn't practiced sounds like a great idea. However, there is no database involed per se, this technique is simulating the moves until the very end of the game, so non-traditional &quot;weird&quot; moves won't throw it off. It is also very important to know that the structure of AlphaGo is not like Deep Blue for chess. Deep Blue was specifically designed to maximize metrics that are likely to lead to victory, such as pawn advantage, king safety, tempo and more. AlphaGo doesn't do any of that. It is a general technique that can learn to solve a large number of different problems. I cannot overstate the significance of this. Almost the entirety of computer science research revolves around creating algorithms that are specifically tailored to one task. Different task, different research projects, different algorithm. Imagine how empowering it would be to have a general algorithm that can solve a large amount of problems. It's incredible! Just as people who don't speak a word of Chinese can write an artificial intelligence program to recognize handwritten Chinese text, someone who hasn't played more than a few games can write a chess or Go program that is beyond the skill of most professional players. This is a wonderful testament of the power of mathematics and science. It was quite surprising to see that AlphaGo played seemingly suboptimal moves when it was ahead to reduce variance and maximize its chance of victory. Take a look at at DeepMind's other technique by the name Deep Q-Learning that plays space invaders on a superhuman level. This shot, at first, looks like a blunder, but if you wait it out, you'll see how brilliant it really is. A move that seems like a blunder at a time may be the optimal move in the grand scheme of things. It not a blunder. It is a move from someone whose brilliance is way beyond the capabilities of even the best human players. There is an excellent analysis of this phenomenon on the Go reddit, I've put a link in the description box, check it out. I'd like to emphasize that the technique learns at first, by looking at a large number of games by amateurs. But the question is, how can it get beyond the level of amateurs? After looking at these games, it will learn the basics and will play millions of games against itself and learn from them. And, to be emphasized: nothing in this algorithm is specific to Go. Nothing. It can be used to solve a number of different problems without significant changes. It would be immensely difficult to overstate the significance of that. Shoutout to Brady Daniels who has an excellent Go educational channel. He has very fluid, enjoyable and understandable explanations, highly recommended, check it out. There is a link to one of his videos in the description box. It is a possibility that the first Go grandmaster to reach 10 dans may not be a human, but a computer. My mind is officially blown. Insanity. One more cobblestone has been laid on the path to artificial general intelligence. This achievement I find to be of equivalent magnitude to landing on the Moon. And this is just the beginning. I can't wait to see this technique being used for research in medicine. Huge respect for Demis Hassabis and Lee Sedol, who were both respectful and humble both in victory, and in defeat. They are true champions of their craft. Thanks so much for DeepMind for creating this rivetingly awesome event. My daughter, Jázmin was born one day before this glorious day. What an exciting time to be alive! Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "3D Depth From a Single Photograph",
            "url": "https://www.youtube.com/watch?v=ZolWxY4f9wc",
            "date": "2016-03-20",
            "tags": [],
            "papers": ["http://www.cs.cornell.edu/~asaxena/learningdepth/saxena_ijcv07_learningdepth.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work tries to estimate depth information from an input photograph. This means that it looks at the photo and tries to tell how far away parts of the image are from the camera. An example output looks like this: on the left, there is an input photograph, and on the right, you see a heatmap with true distance information. This is what we're trying to approximate. This means that we collect a lot of indoor and outdoor images with their true depth information, and we try to learn the correspondence, how they relate to each other. Sidewalks, forests, buildings, you name it. These image and depth pairs can be captured by mounting 3D scanners on this awesome custom-built vehicle. And, gentlemen, that is one heck of a way of spending research funds. The final goal is that we provide a photograph for which the depth information is completely unknown and we ask the algorithm to provide it for us. Here you can see some results: the first image is the input photograph, the second shows the true depth information. The third image is the depth information that was created by this technique. And here is a bunch of results for images downloaded from the internet. It probably does at least as good as a human would. Spectacular. This sounds like a sensorial problem for humans, and a perilous journey for computers to say the least. What is quite remarkable is that these relations can be learned by a computer algorithm. What can we use this for? Well, a number of different things, one of which is to create multiple views of this 2D photograph using the guessed depth information. It can also be super helpful in building robots that can wander about reliably with inexpensive consumer cameras mounted on them. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Decision Trees and Boosting, XGBoost",
            "url": "https://www.youtube.com/watch?v=0Xc9LIb_HTw",
            "date": "2016-03-24",
            "tags": [],
            "papers": ["http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/boosting-icml.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A decision tree is a great tool to help making good decisions from a huge bunch of data. The classical example is when we have a bunch of information about people and would like to find out whether they like computer games or not. Note that this is a toy example for educational purposes. We can build the following tree: if the person's age in question is over 15, the person is less likely to like computer games. If the subject is under 15 and is a male, he is quite likely to like video games, if she's female, then less likely. Note that the output of the tree can be a decision, like yes or no, but in our case, we will assign positive and negative scores instead. You'll see in a minute why that's beneficial. But this tree wa s just one possible way of approaching the problem, and admittedly, not a spectacular one - a different decision tree could be simply asking whether this person uses a computer daily or not. Individually, these trees are quite shallow and we call them weak learners. This term means that individually, they are quite inaccurate, but slightly better than random guessing. And now comes the cool part. The concept of tree boosting means that we take many weak learners and combine them into a strong learner. Using the mentioned scoring system instead of decisions also makes this process easy and straightforward to implement. Boosting is similar to what we do with illnesses. If a doctor says that I have a rare condition, I will make sure and ask at least a few more doctors to make a more educated decision about my health. The cool thing is that the individual trees don't have to be great, if they give you decisions that are just a bit better than random guessing, using a lot of them will produce strong learning results. If we go back to the analogy with doctors, then if the individual doctors know just enough not to kill the patient, a well-chosen committee will be able to put together an accurate diagnosis for the patient. An even cooler, adaptive version of this technique brings in new doctors to the committee according to the deficiencies of the existing members. One other huge advantage of boosted trees over neural networks is that we actually see why and how the computer arrives to a decision. This is a remarkably simple method that leads to results of very respectable accuracy. A well-known software library called XGBoost has been responsible for winning a staggering amount of machine learning competitions in Kaggle. I'd like to take a second to thank you Fellow Scholars for your amazing support on Patreon and making Two Minute Papers possible. Creating these episodes is a lot of hard work and your support has been invaluable so far, thank you so much! We used to have three categories for supporters. Undergrad students get access to a Patron-only activity feed and get to know well in advance the topics of the new episodes. PhD students who are addicted to Two Minute Papers get a chance to see every episode up to 24 hours in advance. Talking about committees in this episode, Full Professors form a Committee to decide the order of the next few episodes. And now, we introduce a new category, the Nobel Laureate. Supporters in this category can literally become a part of Two Minute Papers and will be listed in the video description box in the upcoming episodes. Plus all of the above. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Overfitting and Regularization For Deep Learning",
            "url": "https://www.youtube.com/watch?v=6aF9sJrzxaM",
            "date": "2016-03-30",
            "tags": [],
            "papers": ["http://statweb.stanford.edu/~tibs/lasso/lasso.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In machine learning, we often encounter classification problems where we have to decide whether an image depicts a dog or a cat. We'll have an intuitive, but simplified example where we imagine that the red dots represent dogs, and the green ones are the cats. We first start learning on a training set, which means that we get a bunch of images that are points on this plane, and from these points we try to paint the parts of the plane red and green. This way, we can specify which regions correspond to the concept of dogs and cats. After that, we'll get new points that we don't know anything about, and we'll ask the algorithm, for instance, a neural network to classify these unknown images, so it tells us whether it thinks that it is a dog or a cat. This is what we call a test set. We have had a lots of fun with neural networks and deep learning in previous Two Minute Papers episodes, I've put some links in the description box, check them out! In this example, it is reasonably easy to tell that the reds roughly correspond to the left, and the greens to the right. However, if we just jumped on the deep learning hype train, and don't know much about a neural networks, we may get extremely poor results like this. What we see here is the problem of overfitting. Overfitting means that our beloved neural network does not learn the concept of dogs or cats, it just tries to adapt as much as possible to the training set. As an intuition, think of poorly made real-life exams. We have a textbook where we can practice with exercises, so this textbook is our training set. Our test set is the exam. The goal is to learn from the textbook and obtain knowledge that proves to be useful at the exam. Overfitting means that we simply memorize parts of the textbook instead of obtaining real knowledge. If you're on page 5, and you see a bus, then the right answer is B. Memorizing patterns like this, is not real learning. The worst case is if the exam questions are also from the textbook, because you can get a great grade just by overfitting. So, this kind of overfitting has been a big looming problem in many education systems. Now the question is, which kind of neural network do we want? Something that works like a lazy student, or one that can learn many complicated concepts. If we're aiming for the latter, we have to combat overfitting, which is the bane of so many machine learning techniques. Now, there's several ways of doing that, but today we're going to talk about one possible solution by the name L1 and L2 regularization. The intuition of our problem is that the deeper and bigger neural networks we train, the more potent they are, but at the same time, they get more prone to overfitting. The smarter the student is, the more patterns he can memorize. One solution is to hurl a smaller neural network at the problem. If this smaller version is powerful enough to take on the problem, we're good. A student who cannot afford to memorize all the examples is forced to learn the actual underlying concepts. However, it is very possible that this smaller neural network is not powerful enough to solve the problem. So we need to use a bigger one. But, bigger network, more overfitting. Damn. So what do we do? Here is where L1 and L2 regularization comes to save the day. It is a tool to favor simpler models instead of complicated ones. The idea is that the simpler the model is, the better it transfers the textbook knowledge to the exam, and that's exactly what we're looking for. Here you see images of the same network with different regularization strengths. The first one barely helps anything and as you can see, overfitting is still rampant. With a stronger L2 regularization, you see that the model is simplified substantially, and is likely to perform better on the exam. However, if we add more regularization, it might be that we simplified the model too much, and it is almost the same as a smaller neural network that is not powerful enough to grasp the underlying concepts of the exam. Keep your neural network as simple as possible, but not simpler. One has to find the right balance which is an art by itself, and it shows that training deep neural networks takes a bit of expertise. It is more than just a plug and play tool that solves every problem by magic. If you want to play with the neural networks you've seen in this video, just click on the link in the description box. I hope you'll have at least as much fun with it as I had! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "From Doodles To Paintings With Deep Learning",
            "url": "https://www.youtube.com/watch?v=jMZqxfTls-0",
            "date": "2016-04-04",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1603.01768v1.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is the first paper in Two Minute Papers that showcased so stunning results that people called it out to be an April Fools' day joke. It is based on a deep neural network, and the concept is very simple: you choose an artistic style, you make a terrible drawing, and it creates a beautiful painting out of it. If you would like to know more about deep neural networks, we've had a ton of fun with them in previous episodes, I've put a link to them in the description box! I expect an onslaught of magnificent results with this technique to appear very soon. It is important to note that one needs to create a semantic map for each artistic style so that the algorithm learns the correspondence between the painting and the semantics. However, these maps have to be created only once and can be used forever, so I expect quite a few of them to show up in the near future, which greatly simplifies the workflow. After that, these annotations can be changed at will, you press a button, and the rest is history. Whoa! Wicked results. Some of these neural art results are so good that we should be creating a new class of Turing tests for paintings. This means that we are presented with two images, one of them is painted by a human, and one by a computer. We need to click the ones that we think were painted by a human. Damn, curses! As always, these techniques are new and heavily experimental, and this usually means that they take quite a bit of time to compute. The presentation video you have seen was sped up considerably. If these works are worthy of further attention, and I definitely think they are, then we can expect great strides towards interactivity in followup papers very soon. I am really looking forward to it and we Fellow Scholars will have a ton of fun with these tools in the future. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "The Dunning-Kruger Effect",
            "url": "https://www.youtube.com/watch?v=4Y7RIAgOpn0",
            "date": "2016-04-10",
            "tags": [],
            "papers": ["http://www.nottingham.ac.uk/~ntzcl1/literature/metacognition/kruger.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This episode is about a classic, the Dunning-Kruger effect. I wonder how we could go on for almost 60 Two Minute Papers episodes without the Dunning-Kruger effect? Here is the experiment: participants were tested in different subjects, their test scores were computed, and at the same time, without the scores, they were asked to assess their perceived performance. The test subjects were humor, grammar, and logic. Things, of course, everyone excels at ... or do they? And here is the historic plot with the results. Such a simple plot, yet it tells us so much about people. From left to right, people were ordered by their score as you see with the dotted line. And the other line with the squares shows their perceived score, what they thought their scores would be. People from the bottom 10 percent, the absolute worst performers are convinced that they were well above the average. Competent people, on the other hand, seemed to underestimate their skills. Because the test was easy for them, they assumed that it was easy for everyone else. The extreme to the left is often referred to as the Dunning-Kruger effect, and the extreme to the right, maybe if you imagine the lines extending way-way further, is a common example of impostor syndrome. By the way, everyone thinks they are above average, which is neat mathematical anomaly. We would expect that people who perform poorly should know that they perform poorly, and people who're doing great should know they're doing great. One of the conclusions is that this is not the case, not the case at all. The fact that incompetent people are completely ignorant about their own inadequacy, at first, sounds like such a surprising conclusion. But if we think about it, we find there's nothing surprising about this. The more skilled we are, the more adept we are at estimating our skill level. By gaining more competence, incompetent people also obtained the skill to recognize their own shortcomings. A fish, in the world of Poker, means an inadequate player who is to be extorted by the more experienced. Someone asked how to recognize who the fish is at a poker table. The answer is a classic: if you don't know who the fish is at the table, it is you. The knowledge of the Dunning-Kruger effect is such a tempting tool to condemn other people for their inadequacy. But please, try to resist the temptation, remember, it doesn't help, that's the point of the paper! It is a much more effective tool for our own development if we attempt to use it on ourselves. Does it hurt a bit more? Oh yes, it does! The results of this paper solidify the argument that we need to be very vigilant about our own shortcomings. This knowledge endows you with a shield against ignorance. Use it wisely. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Narrow Band Liquid Simulations",
            "url": "https://www.youtube.com/watch?v=nfPBT71xYVQ",
            "date": "2016-04-28",
            "tags": [],
            "papers": ["https://wwwcg.in.tum.de/fileadmin/user_upload/Lehrstuehle/Lehrstuhl_XV/Research/Publications/2016/NBFlip/nbflip.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Our endeavors in creating amazingly detailed fluid simulations is often hamstrung by the fact that we need to simulate the motion of tens of millions of particles. Needless to say this means excruciatingly long computation times and large memory consumption. This piece of work tries to alleviate the problem by confining the usage of particles to a narrow band close to the liquid surface and thus, decimating the number of particles used in the simulation. The rest of the simulation is computed on a very coarse grid, where we compute quantities of the fluid like velocity and pressure in gridpoints, and instead of computing them everywhere, we try to guess what is happening between these gridpoints. The drawback of this is that we may miss a lot of details because of that. And the brilliant part of this new technique is that we only use a cheap, sparse grid where there is not a lot of things happening, and use the expensive particles only near the surface, where there are a lot of details we can capture well. The FLIP term that you see in the video means Fluid Implicit Particle, a popular way of simulating fluids that uses both grids and particles. In this scene, the old method uses 24 million particles, while the new technique uses only one million, and creates closely matching results. You can see a lot of excess particles in the footage with the classical simulation technique, and the foamish-looking version is the proposed new, more efficient algorithm. Creating such a technique is anything but trivial. Unless special measures are taken, the simulation may have robustness issues, which means that there are situations where it does not produce a sensible result. This is demonstrated in a few examples where with the naive version of the technique, a piece of fluid never ever comes to rest, or it may exhibit behaviors that are clearly unstable. It also takes approximately half as much time to run the simulation, and uses half as much memory, which is such a huge relief for visual  effects artists. I don't know about you Fellow Scholars, but I see a flood of amazing fluid papers coming in the near future and I'm having quite a bit of trouble containing my excitement. Exciting times are ahead indeed. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Training Deep Neural Networks With Dropout",
            "url": "https://www.youtube.com/watch?v=LhhEv1dMpKE",
            "date": "2016-05-01",
            "tags": [],
            "papers": ["https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A quick recap for the Fellow Scholars out there who missed some of our earlier episodes. A neural network is a machine learning technique that was inspired by the human brain. It is not a brain simulation by any stretch of the imagination, but it was inspired by the inner workings of the human brain. We can train it on input and output pairs like images, and descriptions, whether the images depict a mug or a bus. The goal is that after training, we would give unknown images to the network and expect it to recognize whether there is a mug or a bus on them. It may happen that during training, it seems that the neural network is doing quite well, but when we provide the unknown images, it falters and almost never gets the answer right. This is the problem of overfitting, and intuitively, it is a bit like students who are not preparing for an exam by obtaining useful knowledge, but students who prepare by memorizing answers from the textbook instead. No wonder their results will be rubbish on a real exam! But no worries, because we have dropout, which is a spectacular way of creating diligent students. This is a technique where we create a network where each of the neurons have a chance to be activated or disabled. A network that is filled with unrealiable units. And I really want you to think about this. If we could have a system with perfectly reliable units, we should probably never go for one that is built from less reliable units instead. What is even more, this piece of work proposes that we should cripple our systems, and seemingly make them worse on purpose. This sounds like a travesty. Why would anyone want to try anything like this? And what is really amazing is that these unreliable units can potentially build a much more useful system that is less prone to overfitting. If we want to win competitions, we have to train many models and average them, as we have seen with the Netflix prize winning algorithm in an earlier episode. It also relates back to the committee of doctors example that is usually more useful than just asking one doctor. And the absolutely amazing thing is that this is exactly what dropout gives us. It gives the average of a very large number of possible neural networks, and we only have to train one network that we cripple here and there to obtain that. This procedure, without dropout, would normally take years and such exorbitant timeframes to compute, and would also raise all kinds of pesky problems we really don't want to deal with. To engage in modesty, let's say that if we are struggling with overfitting, we could do a lot worse than using dropout. It indeed teaches slacking students how to do their homework properly. Please keep in mind using dropout also leads to longer training times, my experience has been between 2 to 10x, but of course, it heavily depends on other external factors. So it is indeed true that dropout is slow compared to training one network, but it is blazing fast at what it actually approximates, which is training an exponential number of models. I think dropout is one of the greatest examples of the beauty and the perils of research, where sometimes the most counterintuitive ideas give us the best solutions. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Face2Face: Real-Time Facial Reenactment",
            "url": "https://www.youtube.com/watch?v=_S1lyQbbJM4",
            "date": "2016-05-04",
            "tags": [],
            "papers": ["http://www.graphics.stanford.edu/~niessner/papers/2016/1facetoface/thies2016face.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There was a previous episode on a technique where the inputs were a source video of ourselves, and a target actor. And the output was a video of this target actor with our facial gestures. With such an algorithm, one can edit pre-recorded videos in real time, and the current version only needs a consumer webcam to do that. This new version addresses two major shortcomings: One: the previous work relied on depth information, which means that we needed to know how far different parts of the image were from the camera. This newer version only relies on color information and does not need anything beyond that. Whoa! Two: Previous techniques often resorted to copying the footage from the mouth and adding synthetic proxies for teeth. Not anymore with this one! I tip my hat to the authors, who came up with a vastly improved version of their previous method so quickly, and it is probably needless to say that the ramifications of such an existing technique are far reaching, and are hopefully pointed in a positive direction. However, we should bear in mind that from now on, we may be one step closer to an era where a video of something happening won't be taken as proper evidence. I wonder how this will affect legal decision-making in the future. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Deep Learning and Cancer Research",
            "url": "https://www.youtube.com/watch?v=5PSWr2ovBvU",
            "date": "2016-05-08",
            "tags": [],
            "papers": ["http://www.nature.com/articles/srep21471.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's try to assess the workflow of this piece of work in the shortest possible form. The input is images of cells, and the output of the algorithm is a decision that tells us which one of these are cancer cells. As the pipeline of the entire experiment is quite elaborate, we'll confine ourselves to discuss the deep learning-related step at the very end. Techniques prior to this one involved adding chemicals to blood samples. The problem is that these techniques were not so reliable, and that they also destroyed the cells, so it was not possible to check the samples later. As the title of the paper says, it is a label-free technique, therefore it can recognize cancer cells without any intrusive changes to the samples. The analysis happens by simply looking at them. To even have a chance at saying anything about these cells, domain experts have designed a number of features that help us making an educated decision. For instance, they like to look at refractive indices, that tell us how much light slows down when passing through cells. Light absorption and scattering properties are also recognized by the algorithm. Morphological features are also quite important as they describe the shape of the cells and they are among the most useful features for the detection procedure. So, the input is an image, then come the high level features, and the neural networks help locating the cancer cells by learning the relation of exactly what values for these high-level features lead to cancer cells. The proposed technique is significantly more accurate and consistent in the detection than previous techniques. It is of utmost importance that we are able to do something like this on a mass scale because the probability of curing cancer depends greatly on which phase we can identify it. One of the most important factors is early detection and this is exactly how deep learning can aid us. To demonstrate how important early detection is, have a look at this chart of the ovarian cancer survival rates as a function of how early the detection takes place. I think the numbers speak for themselves, but let's bluntly state the obvious: it goes from almost surely surviving to almost surely dying. By the way, they were using L2 regularization to prevent overfitting in the network. We have talked about what each of these terms mean in a previous episode, I've put a link for that in the description box. 95% success rate with the throughput of millions of cells per second. Wow, bravo. A real, Two Minute Papers style hat tip to the authors of the paper. It is really amazing to see different people from so many areas working together to defeat this terrible disease. Engineers create instruments to be able to analyze blood samples, doctors choose the most important features, and computer scientists try to find out the relation between the features and illnesses. Great strides have been made in the last few years, and I am super happy to see that even if you're not a doctor and you haven't studied medicine, you can still help in this process. That's quite amazing. A big shoutout to Kram who has been watching Two Minute Papers since the very first episodes and his presence has always been ample with insightful comments. Thanks for being around! And also, thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Real-Time Shading With Area Light Sources",
            "url": "https://www.youtube.com/watch?v=SC0D7aJOySY",
            "date": "2016-05-11",
            "tags": [],
            "papers": ["https://docs.google.com/uc?export=download&confirm=LArc&id=0BzvWIdpUpRx_d09ndGVjNVJzZjA"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In computer graphics, we use the term shading to describe the process of calculating the appearance of a material. This gives the heart and soul of most graphical systems that visualize something on our screen. Let the blue sphere be the object to be shaded and the red patch be the light source illuminating it. The question is, in this configuration, how should the blue sphere look in reality? In order to obtain high-quality images, we need to calculate how much of the red patch is visible from the blue sphere. This describes the object's relation to the light source. It it close or is it nearby? Is it facing the object or not? What shape is the light source? These factors determine how much light will arrive to the surface of the blue sphere. This is what mathematicians like to call an integration problem. However, beyond this calculation we also have to take into consideration the reflectance of the material that the blue sphere is made of. Whether we have a white wall surface or an orange makes a great deal of difference and throws a wrench in our already complex calculations. The final shading is the product of this visibility situation and the material properties of the sphere. Needless to say that the mathematical description of many materials can get extremely complex, which makes our calculations really time consuming. In this piece of work, a technique is proposed that can approximate these two factors in real time. The paper contains a very detailed demonstration of the difference between this and the analytical computations that give us the perfect results but take extremely long. In short, this technique is very closely matching the analytic results, but it is doing it in real time. I really don't know what to say. We're used to wait for hours to obtain images like this, and now, 15 milliseconds per frame. What a hefty value proposition for a paper. Absolutely spectacular. Some of the results really remind me of topological calculations. Topology is a subfield of mathematics that studies what properties of different shapes are preserved when these shapes are undergoing deformations. It's super useful because, for instance, if we can prove that light behaves in some way when the light source has the shape of a disk, then if we're interested in other shapes, topology can help us determine whether all these enormous books full of theorems on other shapes are going to apply to this shape or not. It may be that we don't need to invent anything and can just use this vast existing knowledge base. Some of the authors of this paper work at Unity, which means that we can expect these awesome results to appear in the video games of the future. Some code and demos are also available on their website which I've linked in the description box, make sure to check them out! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Separable Subsurface Scattering",
            "url": "https://www.youtube.com/watch?v=72_iAlYwl0c",
            "date": "2016-05-15",
            "tags": [],
            "papers": ["http://www.iryoku.com/separable-sss/downloads/Separable-Subsurface-Scattering.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that a portion of incoming light penetrates the surface of a material. Our skin is a little known, but nonetheless great example of that, but so are plant leaves, marble, milk, or snails, to have a wackier example. Subsurface scattering looks unbelievably beautiful, but at the same time, it is very expensive to compute because we have to simulate up to thousands and thousands of light scattering events for every ray of light. And we have to do this for millions of rays. It really takes forever. The lack of subsurface scattering is the reason why we've seen so many lifeless, rubber-looking human characters in video games and animated movies for decades now. This technique is a collaboration between the Activision Blizzard game development company, the University of Zaragoza in Spain, and the Technical University of Vienna in Austria. And, it can simulate this kind of subsurface light transport in half a millisecond per image. Let's stop for a minute and think about this. Earlier, we talked about subsurface scattering techniques that were really awesome, but still took at least let's say four hours on a scene before they became useful. This one is half a millisecond per image. Almost nothing. In one second, it can do this calculation two thousand times. Now, this has to be a completely different approach than just simulating many millions of rays of light, right? We can't take a four hour long algorithm, do some magic and get something like this. The first key thought is that we can set up some cool experiment where we play around with light sources and big blocks of translucent materials, and record how light bounces off of these materials. Cool thing number one: we only need to do it once per material. Number two: the results can be stored in an image. This is what we call a diffusion profile and this is how it looks like. So we have an image of the diffusion profile, and one image of the material that we would like to add subsurface scattering to. This is a convolution-based technique, which means that it enables us not to add these two images together, but to mix them together in a way that the optical properties of the diffusion profiles are carried to the image. If we add the optical properties of an apple to a human face, it will look more like a face that has been carved out of a giant apple. A less asinine application is, of course, if we mix it with the appropriate skin profile image, then we'll get photorealistic looking faces, as it is demonstrated quite aptly by this animation. This apple to skin example, by the way, you can actually try for yourself, as the source code and an executable demo is also freely available for everyone to experiment with. Convolutions have so many cool applications, I don't even know where to start. In fact, I think we should have an episode solely on that. Can't wait, it's going to be a lot of fun! These convolution computations are great, but they are still too expensive for real-time video games. What this work gives us, is a set of techniques that are able to compute this convolution not on these original images, but much smaller, tiny-tiny strips which are much cheaper, but the result of the computations look barely distinguishable. Another cool thing is that the quality of the results is not only scientifically provable, but this technique also opens up the possibility of artistic manipulation. It is done in a way that we can start out with a physically plausible result and tailor it to our liking. You can see some exaggerated examples of that. The entire technique is so simple, a computer program that executes it can fit on your business card. It also seems to have appeared in Blender recently. Also, a big hello and a shoutout for the awesome people at Intel who recently invited my humble self to chat a bit about this technique. If you would like to hear more about the details on how this algorithm works, I have put some videos in the description box. The most important take home message from this project, at least for me, is that it is possible to conduct academic research projects together with companies, and create results that can make it to multi-million dollar computer games, but also having proven results that are useful for the scientific community. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Deep Reinforcement Terrain Learning",
            "url": "https://www.youtube.com/watch?v=wBrwN4dS-DA",
            "date": "2016-05-19",
            "tags": [],
            "papers": ["http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This is a followup work to a technique we have talked about earlier. We have seen how different creatures learned to walk, and their movement patterns happened to be robust to slight variations in the terrain. In this work, we imagine these creatures as a collection of joints and links, typically around 20 links. Depending on what actions we choose for these individual body parts in time, we can construct movements such as walking, or leaping forward. However, this time, these creatures not only learn to walk, but they also monitor their surroundings and are also taught to cope with immense difficulties that arise from larger terrain differences. This means that they learn both on character features, like where the center of mass is and what the velocity of different body parts are, and terrain features, such as, what the displacement of the slope we're walking up on is or if there's a wall ahead of us. The used machinery to achieve this is deep reinforcement learning. It is therefore a combination of a deep neural network and a reinforcement learning algorithm. The neural network learns the correspondence between these states and output actions, and the reinforcement learner tries to guess which action will lead to a positive reward, which is typically measured as our progress on how far we got through the level. In this footage we can witness how a simple learning algorithm built from these two puzzle pieces can teach these creatures to modify their center of mass and adapt their movement to overcome more sophisticated obstacles, and, other kinds of advertisites. And please note that the technique still supports a variety of different creature setups. One important limitation of this technique is that it is restricted to 2D. This means that the characters can walk around not in a 3D world, but on a plane. A question whether we're shackled by the 2D-ness of the technique or if the results can be applied to 3D remains to be seen. I'd like to note that candidly discussing limitations is immensely important in research, and the most important thing is often not we can do at this moment, but the long-term potential of the technique, which, I think this work has in abundance. It's very clear that in this research area, enormous leaps are made year by year, and there's lots to be excited about. As more papers are published on this locomotion problem, the authors also discuss that it would be great to have a unified physics system and some error metrics so that we can measure these techniques against each other on equal footings. I feel that such a work would provide fertile grounds for more exploration in this area, and if I see more papers akin to this one, I'll be a happy man. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Artistic Style Transfer For Videos",
            "url": "https://www.youtube.com/watch?v=Uxax5EKg0zA",
            "date": "2016-05-22",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1604.08610v1", "https://arxiv.org/pdf/1604.08610v1.pdf"],
            "implementations": ["https://github.com/manuelruder/artistic-videos"],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have previously talked about a technique that used a deep neural network to transfer the artistic style of a painting to an arbitrary image, for instance, to a photograph. As always, if you're not familiar with some of these terms, we have discussed them in previous episodes, and links are available in the description box, make sure to check them out! Style transfer is possible on still images. As there is currently no technique to apply this to videos, it is hopefully abundantly clear that a lot of potential still lies dormant inside. But can we apply this artistic style transfer to videos? Would it work if we would simply try? For an experienced researcher, it is flagrantly obvious that it's an understatement to say that that it wouldn't work. It would fail in a spectacular manner, as you can see here. But with this technique, it apparently works quite well. To be frank, the results look gorgeous. So how does it work? Now, don't be afraid, you'll be presented with a concise, but deliberately obscure statement: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Now, the only question is what temporal coherence and optical flow means. Temporal coherence is a term that was used by physicists to describe, for instance, how the behavior of a wave of light changes, or stays the same if we observe it at different times. In computer graphics, it is also an important term because oftentimes, we have techniques that we can apply to one image, but not necessarily to a video, because the behavior of the technique changes drastically from frame to frame, introducing a disturbing flickering effect that you can see in this video here. We have the same if we do the artistic style transfer, because there is no communication between the individual images of the video. The technique has no idea that most of the time we're looking at the same things, and if so, the artistic style would have to be applied the same way over and over to these regions. We are clearly lacking temporal coherence. Now, onto optical flows. Imagine a flying drone that takes a series of photographs while hovering and looking around above us. To write sophisticated navigation algorithms, the drone would have to know which object is which across many of these photographs. If we have slightly turned, most of what we see is the same, and only a small part of the new image is new information. But the computer doesn't know that, as all it sees is a bunch of pixels. Optical flow algorithms help us achieving this by describing the possible motions that give us photograph B from photograph A. In this application, what this means is that there is some inter-frame communication, the algorithm will know that if I colored this person this way a moment ago, I cannot drastically change the style of that region on a whim. It is now easy to see why naively applying such techniques to many individual frames would be a flippant attempt to create beautiful, smooth looking videos. So now, it hopefully makes a bit more sense: This technique preserves temporal coherence when applying the artistic style by incorporating the optical flow of the input video. Such great progress in so little time. Loving it. Last time I've mentioned Kram from the comments section, and this time, I'd like to commend Relatedgiraffe for his insightful comments. Thanks for being around and I've definitely learned from you Fellow Scholars! I am really loving the respectful and quality discussions that take place in the comments section, and it is really cool that we can both learn from each other. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Surface-Only Liquids",
            "url": "https://www.youtube.com/watch?v=-rf_MDh-FiE",
            "date": "2016-05-25",
            "tags": [],
            "papers": ["http://www.cs.columbia.edu/cg/surfaceliquids/droplets.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Most of the techniques we've seen in previous fluid papers run the simulation inside the entire volume of the fluids. These traditional techniques scale poorly with the size of our simulation. But wait, as we haven't talked about scaling before, what does this scaling thing really mean? Favorable scaling means that if we have a bigger simulation, we don't have to wait longer for it. Our scaling is fairly normal if we have a simulation twice as big and we need to wait about twice as much. Poor scaling can give us extraordinarily bad deals, such as waiting ten or more times as much for a simulation that is only twice as big. Fortunately, a new class of algorithms is slowly emerging that try to focus more resources on computing what happens near the surface of the liquid, and try to get away with as little as possible inside of the volume. This piece of work shows that most of the time, we can get away with not doing computations inside the volume of the fluid, but only on the surface. This surface-only technique scales extremely well compared to traditional techniques that simulate the entire volume. If a piece of fluid were an apple, we'd only have to eat the peel, and not the whole apple. It's a lot less chewing, right? As a result, the chewing, or the computation, if you will, typically takes seconds per image instead of minutes. A previous technique on narrow band fluid simulations computed the important physical properties near the surface, but in this case, we compute not near the surface, but only on the surface. The difference sounds subtle, but it makes a completely different mathematical background. To make such a technique work, we have to make simplifications to the problem. For instance, one of the simplifications is to make the fluids incompressible. This means that the density of the fluid is not allowed to change. The resulting technique supports simulating a variety of cases such as dripping water, droplet and crown splashes. fluid chains and sheet flapping. I was spellbound by the mathematics written in the paper that is both crystal clear and beautiful in its flamboyancy. This one is such a spectacular paper. It is so good, I had it on my tablet and couldn't wait to get on the train so I could finally read it. The main limitation of the technique is that it is not that useful if we have a large surface to volume ratio, simply because the peel is still a large amount compared to the volume of our apple. We need it the other way around for this technique to be useful, which is true in many cases. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Schrödinger's Smoke",
            "url": "https://www.youtube.com/watch?v=heY2gfXSHBo",
            "date": "2016-06-05",
            "tags": [],
            "papers": ["http://multires.caltech.edu/pubs/SchrodingersSmoke.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. There are two main branches of efficient smoke and fluid simulator programs: Eulerian and Lagrangian techniques. Before we dive into what these terms mean, I'd like to note that we have closed captions available for this series that you can turn on by clicking the cc button at the bottom of the player. With that out of the way, the Eulerian technique means that we have a fixed grid, and the measurement happens in the gridpoints only. We have no idea what happens between these gridpoints. It may sound counterintuitive at first because it has no notion of particles at all. With the Lagrangian technique, we have particles that move around in space, and we measure important quantities like velocity and pressure with these particles. In short, Eulerian - grids, Lagrangian - particles. Normally, the problem with Eulerian simulations is that we don't know what exactly happens between the gridpoints, causing information to disappear in these regions. To alleviate this, they are usually combined with Lagrangian techniques, because if we also track all these particles individually, we cannot lose any of them. The drawback is, of course, that we need to simulate millions of particles, which will take at least a few minutes for every frame we wish to compute. By formulating his famous equation, the Austrian physicist Erwin Schrödinger won the Nobel prize in 1933. In case you're wondering, yes, this is the guy who forgot to feed his cat. There is two important things you should know about the Schrödinger equation: one is that it is used to describe how subatomic particles behave in time, and two, it has absolutely nothing to do with large-scale fluid simulations whatsoever. The point of this work is to reformulate Schödinger's equation in a way that it tracks the density and the velocity of the fluid in time. This way, it can be integrated in a purely grid-based, Eulerian fluid simulator, and we don't need to track all these individual particles one by one, but we can still keep these fine, small-scale details in a way that rivals Lagrangian simulations, but without the huge additional costs. So, the idea is absolutely bonkers, just the thought of doing this sounds so outlandish to me. And it works! Obstacles are also supported by this technique. Many questions still remain, such as how to mix different fluid interfaces together, how to model the forces between them. I do not have the prescience to see the limits of the approach, but I am quite convinced that this direction holds a lot of promise for the future. I cannot wait to play with the code and see some followup works on this! As always, everything is linked in the video description box. The paper is not only absolutely beautifully written, but it is also a really fun paper to read. As I read it, I really loved how a jolt of epiphany ran through me. It is a fantastic feeling when a lightbulb lights up in my mind as I suddenly get to understand something. I think it is the scientist's equivalent of obtaining englightenment. May it happen to you Fellow Scholars often during your journeys! And I get to spend quite a bit of time every day reading fine works like this. It's a good life. I'd like to give a quick shoutout to this really cool website called Short Science, which is a collection of crowdsourced short summaries for scientific papers. Really cool stuff, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Image Colorization With Deep Learning and Classification",
            "url": "https://www.youtube.com/watch?v=MfaTOXxA8dM",
            "date": "2016-06-08",
            "tags": [],
            "papers": ["http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This work is about adding color to black and white images. There were some previous works that tackled this problem, and many of them worked quite well, but there were cases when the results simply didn't make too much sense. For instance, the algorithm often didn't guess what color the fur of a dog should be. If we would give the same task to a human, we could usually expect better results because the human knows what breed the dog is, and what colors are appropriate for that breed. In short, we know what is actually seen on the image, but the algorithm doesn't - it just trains on black and white and colored image pairs and learns how it is usually done without any concept of what is seen on the image. So here is the idea - let's try to get the neural network not only to colorize the image, but classify what is seen on the image before doing that. If we see a dog in an image, it is not that likely to be pink, is it? If we know that we have to deal with a golf course, we immediately know to reach out for those green crayons. This is a novel fusion-based technique. This means that we have a separate neural network for classifying the images and one for colorizing them. The fusion part is when we unify the information in these neural networks so we can create an output that aggregates all this information. And the results, are just spectacular, the additional information on what these images are about really make a huge impact on the quality of the results. Please note that this is by far not the first work on fusion, I've also linked an earlier paper for recognizing objects in videos, but I think this is a really creative application of the same train of thought that is really worthy of our attention. To delight the fellow tinkerers out there, the source code of the project is also available. The supplementary video reveals that temporal coherence is still a problem. This means that every image is colorized separately with no communication. It is a bit like giving the images to colorize one by one to different people, with no overarching artistic direction. The result we'll get this way is a flickery animation. This problem has been solved for artistic style transfer, which we have discussed in an earlier episode, the link is in the description box. There was one future episode planned about plastic deformations. I have read the paper several times, and it is excellent, but I felt that the quality of my presentation was not up there to put it in front of you Fellow Scholars. It may happen in the future, but I had to shelf this one for now. Please accept my apologies for that. In the next episode, we'll continue with OpenAI's great new invention for reinforcement learning. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Hallucinating Images With Deep Learning",
            "url": "https://www.youtube.com/watch?v=hnT-P3aALVE",
            "date": "2016-06-19",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1605.05396v2"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In an earlier episode, we showcased a technique for summarizing images not in a word, but an entire sentence that actually makes sense. If you were spellbound by those results, you'll be out of your mind when you hear this one: let's turn it around, and ask the neural network to have a sentence as an input, and ask it to generate images according to it. Not fetching already existing images from somewhere, generating new images according to these sentences. Create new images according to sentences. Is this for real? This is an idea, that is completely out of this world. A few years ago, if someone proposed such an idea and hoped that any useful result can come out of this, that person would have immediately been transported to an asylum. An important keyword here is &quot;zero shot&quot; recognition. Before we go to the zero part, let's talk about one shot learning. One shot learning means a class of techniques that can learn something from one, or at most a handful of examples. Deep neural networks typically require to see hundreds of thousands of mugs before they can learn the concept of a mug. However, if I show one mug to any of you Fellow Scholars, you will, of course, immediately get the concept of a mug. At this point, it is amazing what these deep neural networks can do, but with the current progress in this area, I am convinced that in a few years, feeding millions of examples to a deep neural network to learn such a simple concept will be considered a crime. Onto zero shot recognition! The zero shot is pretty simple - it means zero training samples. But this sounds preposterous! What it actually means is that we can train our network to recognize birds, tiny things, what the concept of blue is, what a crown is, but then we ask it to show us an image of &quot;a tiny bird with a blue crown&quot;. Essentially, the neural network learns to combine these concepts together and generate new images leaning on these learned concepts. I think this paper is a wonderful testament as to why Two Minute Papers is such a strident advocate of deep learning and why more people should know about these extraordinary works. About the paper - it is really well written, there are quite a few treats in there for scientists: game theory and minimax optimization, among other things. Cupcakes for my brain. We will definitely talk about these topics in later Two Minute Papers episodes, stay tuned! But for now, you shouldn't only read the paper - you should devour it. And before we go, let's address the elephant in the room: the output images are tiny because this technique is very expensive to compute. Prediction: two papers down the line, it will be done in a matter of seconds, two even more papers down the line, it will do animations in full HD. Until then, I'll sit here stunned by the results, and just frown and wonder. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "What Can We Learn From Deep Learning Programs?",
            "url": "https://www.youtube.com/watch?v=ZBWTD2aNb_o",
            "date": "2016-06-22",
            "tags": [],
            "papers": ["https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. I have recently been witnessing a few heated conversations regarding the submission of deep learning papers to computer vision conferences. The forums are up in arms about the fact that despite some of these papers showcased remarkably good results, they were rejected on the basis of, from what I have heard, not adding too much to the tree of knowledge. They argue that we don't understand what is going on in these neural networks and cannot really learn anything new from them. I'll try to understand and rephrase their argument differently. We know exactly how to train a neural network, it's just that as an output of this process, we get a model of something that resembles a brain as a collection of neurons and circumstances under which these neurons are activated. We store these in a file that can take up to several gigabytes, and the best solutions are often not intuitively understandable for us. For instance, in this video, we're training a neural network to classify these points correctly, but what exactly can we learn if we look into these neurons? Now imagine that in practice, we don't have a handful of these boxes, but millions of them, and more complex than the ones you see here. Let's start with a simple example that hopefully helps getting a better grip of this argument. Now, I'll be damned if this video won't be more than a couple minutes, so this is going to be one of those slightly extended Two Minute Paper episodes. I hope you don't mind! The grammatical rules of my native language, a lot of them, are contained in enormous tomes that everyone has to go through during their school years. Rules are important. They give the scaffoldings for constructing sentences that are grammatically correct. Can we explain or even enumerate these rules? Well, unless you are a linguist, the answer is no. Almost no one really remembers more than a few rules, but every native speaker knows how their language should be spoken. And it is because we've heard a lot of sentences that are correct and learned it by heart what makes a proper sentence and what is gibberish. This is exactly what neural networks do - they are trained in a very similar way. In fact, they are so effective at it that if we you try to forcefully insert some of our knowledge in there, the solutions are going to get worse. It is therefore an appropriate time to ask questions like what merits a paper and what do we define as scientific progress. What if we have extremely accurate algorithms where we don't know what is going on under the hood, or simpler, more intuitive algorithms that may be subpar in accuracy. If we have a top tier scientific conference where only a very limited number of papers get accepted, which ones shall we accept? I hope that this question will spark a productive discussion, and hopefully scientific research venues will be more vigilant about this question in the future. Okay, so the question is crystal clear: knowledge or efficiency? How about possible solutions? Can we extract scientific insights out of these neural networks? Model compression is a way to essentially compress the information in this brain-ish thing, this collection of neurons we described earlier. To demonstrate why this is such a cool idea, let's quickly jump to this program by DeepMind that plays Atari games at an amazingly high level. In breakout, the solution program that you see here is essentially an enormous table that describes what the program should do when it sees different inputs. It is so enormous that it has many millions of records in there. A manual of many thousand pages, if you will. It is easy to execute for a computer, but completely impossible for us to understand why and how it works. However, if we intuitively think about the game itself, we could actually write a super simple program in one line of code that would almost be as good as this. All we need to do is try to follow the ball with the paddle. One line of code, and pretty decent results. Not optimal, but quite decent. From such a program, we can actually learn something about the game. Essentially what we could do with these enormous tables, is compressing them into much much smaller ones. Ones that are so tiny, that we can actually build an intuition from them. This way, the output of a machine learning technique wouldn't only be an extremely efficient program, but finally, the output of the procedure would be knowledge. Insight. If you think about it, such an algorithm would essentially do research by itself. At first, it would randomly try experimenting, and after a large amount of observations are collected, these observations would be explained by a small number of rules. That is exactly the definition of research. And perhaps, this is one of the more interesting future frontiers of machine learning research. And by the way, earlier we have talked about a fantastic paper on Neural Programmer Interpreters that also aimed to output complete algorithms that can be directly used and understood. The link is available in the description box. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Procedural Yarn Models for Cloth Rendering",
            "url": "https://www.youtube.com/watch?v=iTRnr6p7iYo",
            "date": "2016-06-26",
            "tags": [],
            "papers": ["http://www.cs.cornell.edu/~kb/publications/SIG16ProceduralYarn.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we're going to talk about a procedural algorithm. But first of all, what does procedural mean? Procedural graphics is an exciting subfield of computer graphics where instead of storing a lot of stuff, information is generated on the fly. For instance, in photorealistic rendering, we're trying to simulate how digital objects would look like in real life. We usually seek to involve some scratches on our digital models, and perhaps add some pieces of bump or dirt on the surface of the model. To obtain this, we can just ask the computer to not only generate them on the fly, but we can also edit them as we desire. We can also generate cloudy skies and many other things where only some statistical properties have to be satisfied, like how many clouds we wish to see and how puffy they should be, which would otherwise be too laborious to draw by hand. We are scholars after all, we don't have time for that. There are also computer games where the levels we can play through are not predetermined, but also generated on the fly according to some given logical constraints. This can mean that a labyrinth should be solvable, or the level shouldn't contain too many enemies that would be impossible to defeat. The main selling point is that such a computer game has potentially an infinite amount of levels. In this paper, a technique is proposed to automatically generate procedural yarn geometry. A yarn is a piece of thread from which we can sew garments. The authors extensively studied parameters in physical pieces of yarns such as twisting and hairyness and tried to match them with a procedural technique. So, for instance, if in a sudden trepidation we wish to obtain a realistic looking piece of cotton, rayon or silk in our light simulation programs, we can easily get a unique sample of a chosen material, which will be very close to the real deal in terms of these intuitive parameters like hairyness. And we can not only get as long or as many of these as we desire, but can also edit them according to our artistic vision. The solutions are validated against photographs and even CT scans. I always emphasize that I really like these papers where the solutions have some connection to real world around us. This one is super fun indeed! The paper is a majestic combination of beautifully written mathematics and amazing looking results. Make sure to have a look! And you know, we always hear these news where other YouTubers have problems with what is going on in their comments section. Well, not here with our Fellow Scholars. Have a look at the comments section of our previous episode. Just absolutely beautiful. I don't even know what to say, it feels like a secret hideout of respectful and scholarly conversations. It's really amazing that we are building a community of Fellow Scholars, humble people who wish nothing else than to learn more. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Fermat Spirals for Layered 3D Printing",
            "url": "https://www.youtube.com/watch?v=6rNcAVr-U4s",
            "date": "2016-07-05",
            "tags": [],
            "papers": ["http://irc.cs.sdu.edu.cn/uploadfile/2016/0519/20160519043134349.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. What are Hilbert curves? Hilbert curves are repeating lines that are used to fill a square. Such curves, so far, have enjoyed applications like drawing zigzag patterns to prevent biting in our tail in a snake game. Or, jokes aside, it is also useful in, for instance, choosing the right pixels to start tracing rays of light in light simulations, or to create good strategies in assigning numbers to different computers in a network. These numbers, by the way, we call IP addresses. These are just a few examples, and they show quite well how a seemingly innocuous mathematical structure can see applications in the most mind bending ways imaginable. So here is one more. Actually, two more. Fermat's spiral is essentially a long line as a collection of low curvature spirals. These are generated by a remarkably simple mathematical expression and we can also observe such shapes in mother nature, for instance, in a sunflower. And the most natural question emerges in the head of every seasoned Fellow Scholar. Why is that? Why would nature be following mathematics, or anything to do with what Fermat wrote on a piece of paper once? It has only been relatively recently shown that as the seeds are growing in the sunflower, they exert forces on each other, therefore they cannot be arranged in an arbitrary way. We can write up the mathematical equations to look for a way to maximize the concentration of growth hormones within the plant to make it as resilient as possible. In the meantime, this force exertion constraint has to be taken into consideration. If we solve this equation with blood sweat and tears, we may experience some moments of great peril, but it will be all washed away by the beautiful sight of this arrangement. This is exactly what we see in nature. And, which happens to be almost exactly the same as a mind-bendingly simple Fermat spiral pattern. Words fail me to describe how amazing it is that mother nature is essentially able to find these solutions by herself. Really cool, isn't it? If our mind wasn't blown enough yet, Fermat spirals can also be used to approximate a number of different shapes with the added constraint that we start from a given point, take an enormously long journey of low curvature shapes, and get back to almost exactly where we started. This, again, sounds like an innocuous little game evoking ill-concealed laughter in the audience as it is presented by as excited as underpaid mathematicians. However, as always, this is not the case at all. Researchers have found that if we get a 3D printing machine and create a layered material exactly like this, the surface will have a higher degree of fairness, be quicker to print, and will be generally of higher quality than other possible shapes. If we think about it, if we wish to print a prescribed object, like this cat, there is a stupendously large number of ways to fill this space with curves that eventually form a cat. And if we do it with Fermat spirals, it will yield the highest quality print one can do at this point in time. In the paper, this is demonstrated for a number of shapes of varying complexities. And this is what research is all about - finding interesting connections between different fields that are not only beautiful, but also enrich our everyday lives with useful inventions. In the meantime, we have reached our first milestone on Patreon, and I am really grateful to you Fellow Scholars who are really passionate about supporting the show. We are growing at an extremely rapid pace and I am really excited to make even more episodes about these amazing research works. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Time Varying Textures",
            "url": "https://www.youtube.com/watch?v=FMHGS8jWtzM",
            "date": "2016-07-13",
            "tags": [],
            "papers": ["http://www.math.tau.ac.il/~dcor/articles/2016/TW.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This research group is known for their extraordinary ideas, and this piece of work is, of course, no exception. This paper is about time varying textures. Have a look at these photographs that were taken at a different time. And the million dollar question is, can we simulate how this texture would look if we were to go forward in time? A texture weathering simulation, if you will. The immediate answer is that of course not. However, in this piece of work, a single input image is taken, and without any user interaction, the algorithm attempts to understand how this texture might have looked in the past. Now, let's start out by addressing the elephant in the room: this problem can obviously not be solved in the general case for any image. However, if we restrict our assumptions to textures that contain a repetitive pattern, then it is much more feasible to identify the weathering patterns. To achieve this, an age map is built where the red regions show the parts that are assumed to be weathered. You can see on the image how these weathering patterns break up the regularity. Leaning on the assumption that if we go back in time, the regions marked with red will recede, and if we go forward in time, they will grow, we can write a really cool weathering simulator that creates results that look like wizardry. Broken glass, cracks, age rings on a wooden surface, you name it. But we can also use this technique to transfer weathering patterns from one image onto another. Textures with multiple layers are also supported, which means that it can handle images that are given as a sum of a regular and irregular patterns. The blue background is regular and quite symmetric, but the &quot;no parking&quot; text is lacking these regularities. And the amazing thing is that the technique still works on such cases. The results are also demonstrated by putting these weathered textures on 3D models so we can see them all in their glory in our own application. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Visually Indicated Sounds",
            "url": "https://www.youtube.com/watch?v=flOevlA9RyQ",
            "date": "2016-07-17",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1512.08512v2"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This name is not getting any easier, is it? It used to be Károly Zsolnai, which was hard enough, and now this... haha. Anyway, let's get started. This technique simulates how different objects in a video sound when struck. We have showcased some marvelous previous techniques that were mostly limited to wooden and plastic materials. Needless to say, there are links to these episodes in the video description box. A convolutional neural network takes care of understanding what is seen in the video. This technique is known to be particularly suited to processing image and video content. And it works by looking at the silent video directly and trying to understand what is going on, just like a human would. We train these networks with input and output pairs - the input is a video of us beating the hell out of some object with a drumstick. The joys of research! And the output is the sound this object emits. However, the output sound is something that changes in time. It is a sequence, therefore it cannot be handled by a simple classical neural network. It is learned by a recurrent neural network that can take care of learning such sequences. If you haven't heard these terms before, no worries, we have previous episodes on all of them in the video description box, make sure to check them out! This piece of work is a nice showcase of combining two quite powerful techniques: the convolutional neural network tries to understand what happens in the input video, and the recurrent neural network seals the deal by learning and guessing the correct sound that the objects shown in the video would emit when struck. The synthesized outputs were compared to the real world results both mathematically and by asking humans to try to tell from the two samples which one the real deal is. These people were fooled by the algorithm around 40% of the time, which I find to be a really amazing result, considering two things: first, the baseline is not 50%, but 0% because people don't pick choices at random - we cannot reasonably expect a synthesized sound to fool humans at any time. Like nice little neural networks, we've been trained to recognize these sounds all our lives, after all. And second, this is one of the first papers from a machine learning angle on sound synthesis. Before reading the paper, I expected at most 10 or 20 percent, if that. The tidal wave of machine learning runs through a number of different scientific fields. Will deep learning techniques establish supremacy in these areas? Hard to say yet, but what we know for sure is that great strides are made literally every week. There are so many works out there, sometimes I don't even know where to start. Good times indeed! Before we go, some delightful news for you Fellow Scholars! The Scholarly Two Minute Papers store is now open! There are two different kinds of men's T-shirts available, and a nice sleek design version that we made for the Fellow Scholar ladies out there! We also have The Scholarly Mug to get your day started in the most scientific way possible. We have tested the quality of these products and were really happy with what we got. If you ordered anything, please provide us feedback on how you liked the quality of the delivery and the products themselves. If you can send us an image of yourself wearing or using any of these, we'd love to have a look. Just leave them in the comments section or tweet at us! If you don't like what you got, within 30 days, you can exchange it or get your product cost refunded. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Photorealistic Images from Drawings",
            "url": "https://www.youtube.com/watch?v=a3sgFQjEfp4",
            "date": "2016-07-20",
            "tags": [],
            "papers": ["http://sketchy.eye.gatech.edu/paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When we were children, every single one of us dreamed about having a magic pencil that would make our adorable little drawings come true. With the power of machine learning, the authors of this paper just made our dreams come true. Here's the workflow: we provide a crude drawing of something, and the algorithm fetches a photograph from a database that depicts something similar to it. It's not synthesizing new images from scratch from a written description like one of the previous works, it fetches an already existing image from a database. The learning happens by showing a deep convolutional neural network pairs of photographs and sketches. If you are not familiar with these networks, we have some links for you in the video description box! It is also important to note that this piece of work does not showcase a new learning technique, it is using existing techniques on a newly created database that the authors kindly provided free of charge to encourage future research in this area. What we need to teach these networks is the relation of a photograph and a sketch. For instance, in an earlier work by the name Siamese networks, the photo and the sketch would be fed to two convolutional neural networks with the additional information whether this pair is considered similar or dissimilar. This idea of Siamese networks was initially applied to signature verification more than 20 years ago. Later, Triplet networks were used provide the relation of multiple pairs, like &quot;this sketch is closer to this photo than this other one&quot;. There is one more technique referred to in the paper that they used, which is quite a delightful read, make sure to have a look! We need lots and lots of these pairs so the learning algorithm can learn what it means that a sketch is similar to a photo, and as a result, fetch meaningful images for us. So, if we train these networks on this new database, this magic pencil dream of ours can come true. What's even better, anyone can try it online! This is going to be a very rigorous and scholarly scientific experiment - I don't know what this should be, but I hope the algorithm does. Well, that kinda makes sense. Thanks, algorithm! For those Fellow Scholars out there who are endowed with better drawing skills than I am, well, basically all of you - if you have tried it and got some amazing, or maybe not so amazing results, please post them in the comments section! Or, as we now have our very own subreddit, make sure to drop by and post some of your results there so we can marvel at them, or have a good laugh at possible failure cases. I am looking forward to meeting you Fellow Scholars at the subreddit. Flairs are also available. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Bundlefusion: 3D Scenes from 2D Videos",
            "url": "https://www.youtube.com/watch?v=zLzhsyeAie4",
            "date": "2016-07-25",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1604.01093v1.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work enables us to walk around in a room with a camera, and create a complete 3D computer model from the video footage. The technique has a really cool effect where the 3D model is continuously refined as we obtain more and more data by walking around with our camera. This is a very difficult problem, and a good solution to this offers a set of cool potential applications. If we have a 3D model of a scene, what can we do with it? Well, of course, assign different materials to them and run a light simulation program for architectural visualization applications, animation movies, and so on. We can also easily scan a lot of different furnitures and create a useful database out of them. There are tons of more applications, but I think these should do for starters. Normally, if one has to create a 3D model of a room or a building, the bottom line is that it requires several days or weeks of labor. Fortunately, with this technique, we'll obtain a 3D model in real time and we won't have to go through these tribulations. However, I'd like to note that the models are still by far not perfect, if we are interested in the many small, intricate details, we have add them back by hand. Previous methods were able to achieve similar results, but they suffer from a number of different drawbacks, for instance, most of them don't support traditional consumer cameras or take minutes to hours to perform the reconstruction. To produce the results presented in the paper, an NVIDIA TITAN X video card was used, which is currently one of the pricier pieces of equipment for consumers, but not so much for companies who are typically interested in these applications. If we take into consideration the rate at which graphical hardware is improving, anyone will be able to run this at home in real time in a few years time. The comparisons to previous works reveal that this technique is not only real time, but the quality of the results is mostly comparable, and in some cases, it surpasses previous methods. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "What is Optimization? + Learning Gradient Descent",
            "url": "https://www.youtube.com/watch?v=1ypV5ZiIbdA",
            "date": "2016-07-29",
            "tags": [],
            "papers": ["http://arxiv.org/pdf/1606.04474v1.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today, we're not going to have the usual visual fireworks that we had with most topics in computer graphics, but I really hope you'll still find this episode enjoyable and stimulating. This episode is also going to be a bit heavy on what optimization is and we'll talk a little bit at the end about the intuition of the paper itself. We are going to talk about mathematical optimization. This term is not to be confused with the word &quot;optimization&quot; that we use in our everyday lives for, for instance, improving the efficiency of a computer code or a workflow. This kind of optimization means finding one, hopefully optimal solution from a set of possible candidate solutions. An optimization problem is given the following way: one, there is a set of variables we can play with, and two, there is an objective function that we wish to minimize or maximize. Well, this probably sounds great for mathematicians, but for everyone else, maybe this is a bit confusing. Let's build a better understanding of this concept through an example! For instance, let's imagine that we have to cook a meal for our friends from a given set of ingredients. The question is, how much salt, vegetables and meat goes into the pan. These are our variables that we can play with, and the goal is to choose the optimal amount of these ingredients to maximize the tastiness of the meal. Tastiness will be our objective function, and for a moment, we shall pretend that tastiness is an objective measure of a meal. This was just one toy example, but the list of applications is endless. In fact, optimization is so incredibly ubiquitous, there is hardly any field of science where some form of it is not used to solve difficult problems. For instance, if we have the plan of a bridge, we can ask it to tell us the minimal amount of building materials we need to build it in a way that it remains stable. We can also optimize the layout of the bridge itself to make sure the inner tension and compression forces line up well. A big part of deep learning is actually also an optimization problem. There are a given set of neurons, and the variables are when they should be activated, and we're fiddling with these variables to minimize the output error, which can be, for instance, our accuracy in guessing whether a picture depicts a muffin or a chihuahua. The question for almost any problem is usually not whether it can be formulated as an optimization problem, but whether it is worth it. And by worth it I mean the question whether we can solve it quickly and reliably. An optimizer is a technique that is able to solve these optimization problems and offer us a hopefully satisfactory solution to them. There are many algorithms that excel at solving problems of different complexities, but what ties them together is that they are usually handcrafted techniques written by really smart mathematicians. Gradient descent is one of the simplest optimization algorithms where we change each of the variables around a bit, and as a result, see if the objective function changes favorably. After finding a direction that leads to the most favorable changes, we shall continue our journey in that direction. What does this mean in practice? Intuitively, in our cooking example, after making several meals, we would ask our guests about the tastiness of these meals. From their responses, we would recognize that adding a bit more salt led to very favorable results, and since these people are notorious meat eaters, decreasing the amount of vegetables and increasing the meat content also led to favorable reviews. And we, of course, on the back of this newfound knowledge, will cook more with these variable changes in pursuit of the best possible meal in the history of mankind. This is something that is reasonably close to what gradient descent is in mathematics. A slightly more sophisticated version of gradient descent is also a very popular way of training neural networks. If you have any questions regarding the gradient part, we had an extended Two Minute Papers episode on what gradients are and how to use them to build an awesome algorithm for light transport. It is available, where? Well, of course, in the video description box, Károly, why are you even asking. So what about the paper part? This incredible new work of Google DeepMind shows that an optimization algorithm itself can emerge as a result of learning. An algorithm itself is not considered the same one thing as deciding what an image depicts or how we should grade a student essay, it is an algorithm, a sequence of steps we have to take. If we're talking about outputting sequences, we'll definitely need to use a recurrent neural network for that. Their proposed learning algorithm can create new optimization techniques that outperform previously existing methods not everywhere, but on a set of specialized problems. I hope you've enjoyed the journey, we'll talk quite a bit about optimization in the future, you'll love it. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Task-based Animation of Virtual Characters",
            "url": "https://www.youtube.com/watch?v=ZHoNpxUHewQ",
            "date": "2016-07-31",
            "tags": [],
            "papers": ["http://www.cs.ubc.ca/~van/papers/2016-TOG-taskBasedLocomotion/2016-TOG-taskBasedLocomotion.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In this piece of work, you'll see wondrous little sequences of animations where a virtual character is asked to write on a whiteboard, move boxes, and perform different kinds of sitting behaviors. The emphasis is on synthesizing believable footstep patterns for this character. This sounds a bit mundane, but we'll quickly realize that the combination and blending of different footstep styles is absolutely essential for a realistic animation of these tasks. Beyond simple locomotion (walking if you will), for instance, sidestepping, using toe and heel pivots, or partial turns and steps every now and then are essential in obtaining a proper posture for a number of different tasks. A rich vocabulary of these movement types and proper transitions between them lead to really amazing animation sequences that you can see in the video. For instance, one of the most heinous examples of the lack of animating proper locomotion we can witness in older computer games, and sometimes even today, is when a character is writing on a whiteboard, who suddenly runs out of space, turns away from it, walks a bit, turns back towards the whiteboard and continues writing there. Even if we have impeccable looking photorealistically rendered characters, such robotic behaviors really ruin the immersion. In reality, a simple sidestepping would do the job, and this is exactly what the algorithm tells the character to perform. Very simple and smooth. This technique works by decomposing a given task to several subtasks, like starting to sit on a box, or getting up, and choosing the appropriate footstep types and transitions for them. One can also mark different tasks as being low or high-effort that are marked with green and blue. A low effort task could mean fixing a minor error on the whiteboard nearby without moving there, and a high effort task that we see marked with blue would be continuing our writing on a different part of the whiteboard. For these tasks, the footsteps are planned accordingly. Really cool. This piece of work is a fine example of the depth and complexity of computer graphics and animation research, and how even the slightest failure in capturing fine scale details is enough to break the immersion of reality. It is also really amazing that we have so many people who are interested in watching these videos about research, and quite a few of you decided to also support us on Patreon. I feel really privileged to have such amazing supporters like you Fellow Scholars. As always, I kindly thank you for this at the end of these videos, so here goes... Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Peer Review and the NIPS Experiment",
            "url": "https://www.youtube.com/watch?v=a1z6GXj8QK8",
            "date": "2016-08-03",
            "tags": [],
            "papers": ["http://blog.mrtz.org/2014/12/15/the-nips-experiment.html"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We are here to answer a simple question: what is peer review? Well, in science, making sure that the validity of published results is beyond doubt is of utmost importance. To this end, many scientific journals and conferences exist where researchers can submit their findings in the form of a science paper. As a condition of acceptance, these papers shall undergo extensive scrutiny by typically 2 to 5 other scientists. This refereeing process we call peer review. Single blind reviewing means that the names of the reviewers are shrouded in mystery, but the authors of the paper are known to them. In double blind reviews, however, the papers are anonymized, and none of the parties know the names of each other. These different kinds of blind reviews were made to eliminate possible people-related biases. There is a lot of discussion whether they do a good job at that or not, but this is what they are for. After the review, if the results are found to be correct, and the reviews are favorable enough, the paper is accepted and subsequently published in a journal and/or presented at a conference. Usually, the higher the prestige of a publication venue is, the higher the likelihood of rejection, which inevitably raises a big question: how to choose the papers that are to be accepted? As we are scientists, we have to try to ensure that the peer review is a fair and consistent process. To measure if this is the case, the NIPS experiment was born. NIPS is one of the highest quality conferences in machine learning with a remarkably low acceptance ratio, which typically hovers below 25%. This is indeed remarkably low considering the fact that many of the best research groups in the world submit their finest works here. So here is the astute idea behind the NIPS experiment: a large amount of papers would be secretly disseminated to multiple committees, they would review it without knowing about each other, and we would have a look whether they would accept or reject the same papers. Re-reviewing papers and see if the results is the same, if you will. At a given prescribed acceptance ratio, there was a disagreement for 57% of the papers. This means that one of the committees would accept the paper and the other wouldn't, and vice versa. Now, to put this number into perspective, the mathematical model of a random committee was put together. This means that the members of this committee have no idea what they are doing, and as a review, they basically toss up a coin and accept or reject the paper based on the result. The calculations conclude that this random committee would have this disagreement ratio of about 77%. This is hardly something to be proud of: the consistency of expert reviewers is significantly closer to a coinflip than to a hypothetical perfect review process. So, experts, 57% disagreement, Coinflip Committee, 77% disagreement. It is not as bad as the Coinflip Committee, so the question naturally arises: where are the differences? Well, it seems that the top 10% of the papers are clearly accepted by both committees, the bottom 25% of the papers are clearly rejected, this is the good news, and the bad news is that anything between might as well be decided with a cointoss. If the consistency of peer review is subject to maximization, we clearly have to do something different. Huge respect for the NIPS organizers for doing this laborious experiment, for the reviewers who did a ton of extra work, and kudos for the fact that the organizers were willing to release such uncomfortable results. This is very important, and is the only way of improving our processes. Hopefully, someday we shall have our revenge over the Coinflip Committee. Can we do something about this? What is a possible solution? Well, of course, this is a large and difficult problem for which I don't pretend to have any perfect solutions, but there is a really interesting idea by a renowned professor about crowdsourcing reviews that I found to be spectacular. I'll leave the blog post in the comments section both for this and the NIPS experiment, and we shall have an entire episode about this soon. Stay tuned! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "The Science of Medal Predictions (2016 Rio Olympics Edition)",
            "url": "https://www.youtube.com/watch?v=gHMY40kEXzs",
            "date": "2016-08-08",
            "tags": [],
            "papers": ["https://www.researchgate.net/profile/Daniel_Johnson7/publication/4920482_A_Tale_of_Two_Seasons_Participation_and_Medal_Counts_at_the_Summer_and_Winter_Olympic_Games/links/0c9605229d43e35dbf000000.pdf?origin=publication_detail"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. The 2016 Rio Olympic Games is right around the corner, so it is the perfect time to talk a bit about how we can use science to predict the results. Before we start, I'd like to mention that we won't be showcasing any predictions for this year's Olympics, instead, we are going to talk about a model that was used to predict the results of previous Olympic Games events. So, the following, very simple question arises: can we predict the future? The answer is very simple: no, we can't. End of video, thanks for watching! Well, jokes aside, we cannot predict the future itself, but we can predict what is likely to happen based on our experience of what happened so far. In mathematics, this is what we call extrapolation. There is also a big difference between trying to extrapolate the results of one athlete, or the aggregated number of medals for many athletes, usually an entire nation. To bring up an example about traffic, if we were to predict where one individual car is heading, we would obviously fail most of the time. However, whichever city we live in, we know exactly the hotspots where there are traffic jams every single morning of the year. We cannot accurately predict the behavior of one individual, but if we increase the size of the problem and predict for a group of people, it suddenly gets easier. Going back to the Olympics, will Usain Bolt win the gold on the 100 meter sprint this year? Predicting the results of one athlete is usually hopeless, and we bravely call such endeavors to be mere speculation. The guy whose results we're trying to predict may not even show up this year - as many of you have heard, many of the Russian athletes have been banned from the Olympic Games. Our model would sure as hell not be able to predict this. Or would it? We'll see in a second, but hopefully it is easy to see that macro-level predictions are much more feasible than predicting on an individual level. In fact, to demonstrate how much of an understatement it is to say feasible, hold onto your seatbelts, because Daniel Johnson, a professor of microeconomics at the Colorado College created a simple prediction model, that, over the past 5 Olympic Games, was able to achieve 94% agreement between the predicted and actual medal counts per nation. What is even more amazing is that the model doesn't even take into consideration the athletic abilities of any of these contenders. Wow! Media articles report that his model uses only 5 simple variables: a country's per-capita income, population, political structure, climate, and host-nation advantage. Now first, I'd first like to mention that GDP per capita means the Gross Domestic Product of one person in a given country, therefore it is independent of the population of the country. If we sit down and read the paper, which is a great and very easy read and you should definitely have a look, it's in the video description box. So, upon reading the paper, we realize there are more variables that are subject to scrutiny: for instance, a proximity factor, which encodes the distance from the hosting nation. Not only the hosting nation itself, but its neighbors are also enjoying significant advantages in the form of lower transportation costs and being used to the climate of the venue. Unfortunately I haven't found his predictions for this year's Olympics, but based on the simplicity of the model, it should be quite easy to run the predictions provided that the sufficient data is available. The take home message is that usually the bigger the group we're trying to predict results for, the lesser the number of variables that are enough to explain their behavior. If we are talking about the Olympics, 5 or 6 variables are enough to faithfully predict nationwide medal counts. These are amazing results that are also a nice testament to the power of mathematics. I also really like how the citation count of the paper gets a big bump every four years. I wonder why? If you are interested in how the Olympic Games unfold, make sure to have a look at the Olympics reddit, I found it to be second to none. As always, the link is available in the description box. Thanks for watching and for your generous support, and I'll see you next time!"
        },
        {
            "name": "What is an Autoencoder?",
            "url": "https://www.youtube.com/watch?v=Rdpbnd0pCiI",
            "date": "2016-08-11",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1312.6114.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. As we have seen in earlier episodes of the series, neural networks are remarkably efficient tools to solve a number of really difficult problems. The first applications of neural networks usually revolved around classification problems. Classification means that we have an image as an input, and the output is, let's say a simple decision whether it depicts a cat or a dog. The input will have as many nodes as there are pixels in the input image, and the output will have 2 units, and we look at the one of these two that fires the most to decide whether it thinks it is a dog or a cat. Between these two, there are hidden layers where the neural network is asked to build an inner representation of the problem that is efficient at recognizing these animals. So what is an autoencoder? An autoencoder is an interesting variant with two important changes: first, the number of neurons is the same in the input and the output, therefore we can expect that the output is an image that is not only the same size as the input, but actually is the same image. Now, this normally wouldn't make any sense, why would we want to invent a neural network to do the job of a copying machine? So here goes the second part: we have a bottleneck in one of these layers. This means that the number of neurons in that layer is much less than we would normally see, therefore it has to find a way to represent this kind of data the best it can with a much smaller number of neurons. If you have a smaller budget, you have to let go of all the fluff and concentrate on the bare essentials, therefore we can't expect the image to be the same, but they are hopefully quite close. These autoencoders are capable of creating sparse representations of the input data and can therefore be used for image compression. I consciously avoid saying &quot;they are useful for image compression&quot;. Autoencoders, offer no tangible advantage over classical image compression algorithms like JPEG. However, as a crumb of comfort, many different variants exist that are useful for different tasks other than compression. There are denoising autoencoders that after learning these sparse representations, can be presented with noisy images. As they more or less know how this kind of data should look like, they can help in denoising these images. That's pretty cool for starters! What is even better is a variant that is called the variational autoencoder that not only learns these sparse representations, but can also draw new images as well. We can, for instance, ask it to create new handwritten digits and we can actually expect the results to make sense! There is an excellent blog post from Francois Cholle, the creator of the amazing Keras library for building and training neural networks, make sure to have a look! With these examples, we were really only scratching the surface, and I expect quite a few exciting autoencoder applications to pop up in the near future as well. I cannot wait to get my paws on those papers. Hopefully you Fellow Scholars are also excited! If you are interested in programming, especially in python, make sure to check out the channel of Sentdex for tons of machine learning programming videos and more. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Neural Material Synthesis",
            "url": "https://www.youtube.com/watch?v=XpwW3glj2T8",
            "date": "2016-08-17",
            "tags": [],
            "papers": ["https://mediatech.aalto.fi/publications/graphics/TwoShotSVBRDF/aittala2015_siggraph.pdf", "https://mediatech.aalto.fi/publications/graphics/NeuralSVBRDF/aittala2016_siggraph.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. If you are new here, this is a series about research with the name Two Minute Papers, but let's be honest here. It's never two minutes. We are going to talk about two really cool papers that help us create physically based material models from photographs that we can use in our light simulation programs. Just as a note, these authors, Miika and Jaakko have been on a rampage for years now and have popped so many fantastic papers each of which I was blown away by. For instance, earlier, we talked about their work on Gradient Domain Light Transport, brilliant piece of work, I've put a link in the description box, make sure to check it out! So the problem we're trying to solve is very simple to understand: the input is a photograph of a given material somewhere in our vicinity, and the output is a bona fide physical material model that we can use in our photorealistic rendering program. We can import real world materials in our virtual worlds, if you will. Before we proceed, let's define a few mandatory terms: A material is diffuse if incoming light from one direction is reflected equally in all directions. This means that they look the same from all directions. White walls and matte surfaces are excellent examples of that. A material we shall consider specular if incoming light from one direction is reflected back to one direction. This means that if we turn our head a bit, we will see something different. For instance, the windshield of a car, water and reflections in a mirror can be visualized with a specular material model. Of course, materials can also be a combination of both. For instance, car paint, our hair and skin are all combinations of these material models. Glossy materials are midway between the two where the incoming light from one direction is reflected to not everywhere equally, and not in one direction, but a small selected set of directions. They change a bit when we move our head, but not that much. In the Two-Shot Capture paper, a material model is given by how much light is reflected and absorbed by the diffuse and specular components of the material, and something that we call a normal map, which captures the bumpiness of the material. Other factors like glossiness and anisotropy are also recorded, but we shall focus on the diffuse and the specular parts. The authors ask us to grab our phone for two photographs of a material to ensure a high-quality reconstruction procedure: one with flash, and one without. And the question immediately arises: why two images? Well, the image without flash can capture the component that looks the same from all directions, this is the diffuse component, and the photograph with flash can capture the specular component because we can see how the material handles specular reflections. And it is needless to say, the presented results are absolutely fantastic. So, first paper, two images, one material model. And therein lies the problem, which they tried to address in the second paper. If a computer looks at such an image, it doesn't know which part of one photograph is the diffuse and which is the specular reflection. However, I remember sitting in the waiting room of a hospital while reading the first paper, and this waiting room had a tiled glossy wall, and I was thinking that one image should be enough, because if I look at something, I can easily discern what the diffuse colors are, and which part is the specular reflection of something else. I don't need multiple photographs for that. I can also immediately see how bumpy it is, even from one photograph, I don't need to turn my head around. This is because we, humans have not a mathematical, but an intuitive understanding of the materials we see around us. So can we explain the same kind of understanding of materials to a computer somehow? Can we do it with only one image? And the answer is, yes we can, and, hopefully, we already feel the alluring call of neural networks. We can get a neural network that was trained on a lot of different images to try to guess what these material reflectance parameters should look like. However, the output should not be one image, but multiple images with the diffuse and specular reflectance informations, and a normal map to describe the bumpiness of this surface. Merely throwing a neural network at this problem, is however, not sufficient. There needs to be some kind of conspiracy between these images, because real materials are not arbitrarily put together. If one of these images is smooth, or has interesting features somewhere, the others have to follow it in some way. This &quot;some way&quot; is mathematically quite challenging to formulate, which is a really cool part of the paper. This conspiracy part is a bit like if we had 4 criminals testifying at a trial, where they try to sell their lies, and to maintain the credibility of their made up story, they have previously had to synchronize their lies so they line up correctly. The paper contains neat tricks to control the output of the neural network and create these conspiracies across these multiple image outputs that yield a valid and believable material model. And the results, are again, just fantastic. Second paper, one image, one material model. It doesn't get any better than that. Spectacular, not specular...spectacular piece of work. The first paper is great, but the second is smoking hot, by all that is holy, I am getting goosebumps. If you are interested in hearing a bit more about light transport and are not afraid of some mathematics, we recently recorded my full course on this at the Technical University of Vienna, the entirety of which is freely available for everyone. There is a link for it in the video description box, make sure to check it out! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "3D Printing With Filigree Patterns",
            "url": "https://www.youtube.com/watch?v=cVZzkSaxKmY",
            "date": "2016-08-25",
            "tags": [],
            "papers": ["http://i.cs.hku.hk/~wkchen/projects/papers/weikai_sig16_filigree_final.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Filigrees are detailed, thin patterns typically found in jewelry, fabrics and ornaments, and as you may imagine, crafting such motifs on objects is incredibly laborious. This project is about leaving out the craftsmen from the equation by choosing a set of target filigree patterns and creating a complex shape out of them that can be easily 3D printed. The challenge lies in grouping and packing up these patterns to fill a surface evenly. We start out with a base model with a poor structure, which is not completely random, but as you can see, is quite a forlorn effort. In several subsequent steps, we try to adjust the positions and shapes of the filigree elements to achieve more pleasing results. The more pleasing results we define as one that minimizes the amount of overlapping, and maximizes the connectivity of the final shape. Sounds like an optimization problem from earlier. And, that is exactly what it is. Really cool, right? The optimization procedure itself is far from trivial, and the paper discusses possible challenges and their solutions in detail. For instance, the fact that we can also add control fields to describe our vision regarding the size and orientation of the filigree patterns is an additional burden that the optimizer has to deal with. We can also specify the ratio of the different input filigree elements that we'd like to see added to the model. The results are compared to previous work, and the difference speaks for itself. However, it's important to point out that even this thing that we call previous work was still published this year. Talk about rapid progress in research! Absolutely phenomenal work. The evaluation and the execution of the solution, as described in the paper is also second to none. Make sure to have a look. And thank so much for taking the time to comment on our earlier video about the complexity of the series. I'd like to assure you we read every single comment and found a ton of super helpful feedback there. It seems to me that a vast majority of you agree that a simple overlay text does the job, and while it is there, it's even better to make it clickable so it leads to a video that explains the concept in a bit more detail for the more curious minds out there. I'll try to make sure that everything is available in mobile as well. You Fellow Scholars are the best and thanks so much for everyone for leaving a comment. Also, please let me know in the comments section if you have found this episode to be understandable or if there were any terms that you've never heard of. If everything was in order, that's also valuable information, so make sure to leave a comment. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Interactive Hair-Solid Simulations",
            "url": "https://www.youtube.com/watch?v=HvHZXPd0Bjs",
            "date": "2016-08-29",
            "tags": [],
            "papers": ["http://gaps-zju.org/mlchai/resources/chai2016adaptive.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have talked about fluid and cloth simulations earlier, but we never really set foot in the domain of hair simulations in the series. To obtain some footage of virtual hair movement, simulating the dynamics of hundreds of thousands of hair strands is clearly too time consuming and would be a flippant attempt to do so. If we do not wish to watch a simulation unfold with increasing dismay as it would take hours of computation time to obtain just one second of footage, we have to come up with a cunning plan. A popular method to obtain detailed real-time hair simulations is not to compute the trajectory of every single hair strand, but to have a small set of strands that we call guide hairs. For these guide hairs, we compute everything. However, since this is a sparse set of elements, we have to fill the gaps with a large number of hair strands, and we essentially try to guess how these should move based on the behavior of guide hairs near them. Essentially, one guide hair is responsible in guiding an entire batch, or an entire braid of hair, if you will. This technique we like to call a reduced hair simulation, and the guessing part is often referred to as interpolation. And the question immediately arises: how many guide hairs do we use and how many total hair strands can we simulate with them without our customers finding out that we're essentially cheating? The selling point of this piece of work, is that it uses only 400 guide hairs, and leaning on them, and it can simulate up to a total number of 150 thousand strands in real time. This leads to amazingly detailed hair simulations. My goodness, look at how beautiful these results are! Not only that, but as it is demonstrated quite aptly here, it can also faithfully handle rich interactions and collisions with other solid objects. For instance, we can simulate all kinds of combing, or pulling our hair out, which is what most researchers do in their moments of great peril just before finding the solution to a difficult problem. Not only hair, but a researcher simulator, if you will. The results are compared to a full space simulation, which means simulating every single hair strand, and that is exactly as time consuming as it sounds. The results are very close to being indistinguishable, which was not the case for previous works that created false intersections where hair strands would erroneously go through solid objects. We can also stroke bunnies with our hair models in this truly amazing piece of work. These episodes are also available in early access for our Patreon supporters. We also have plenty of other really neat perks, I've put a link in the description box, make sure to have a look! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "StyLit, Illumination-Guided Artistic Style Transfer",
            "url": "https://www.youtube.com/watch?v=ksCSL6Ql0Yg",
            "date": "2016-09-01",
            "tags": [],
            "papers": ["http://dcgi.fel.cvut.cz/home/sykorad/Fiser16-SIG.pdf"],
            "implementations": ["https://github.com/manuelruder/artistic-videos"],
            "transcript": []
        },
        {
            "name": "Automatic Hair Modeling from One Image",
            "url": "https://www.youtube.com/watch?v=XmM1tF7AxdA",
            "date": "2016-09-06",
            "tags": [],
            "papers": ["http://gaps-zju.org/mlchai/resources/chai2016autohair.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A couple episodes ago, we finally set sail in the wonderful world of hair simulations. And today we shall continue our journey in this domain. But this time, we are going to talk about hair modeling. So first, what is the difference between hair simulation and modeling? Well, simulation is about trying to compute the physical forces that act on hair strands, thereby showing to the user, how they would move about in reality. Modeling, however, is about obtaining geometry information from a photograph. This geometry information we can use in our movies and computer games. We can also run simulations on them and see how they look on a digital character. Just think about it, the input is one photograph and the output is a digital 3D model. This sounds like a remarkably difficult problem. Typically something that a human would do quite well at, but it would be too labor-intensive to do so for a large number of hairstyles. Therefore, as usual, neural networks enter the fray by looking at the photograph, and trying to estimate the densities and distributions of the hair strands. The predicted results are then matched with the hairstyles found in public data repositories, and the closest match is presented to the user. You can see some possible distribution classes here. Not only that, but the method is fully automatic, which means that unlike most previous works, it doesn't need any guidance from the user to accomplish this task. As a result, the authors created an enormous dataset with 50 thousand photographs and their reconstructions that they made freely available for everyone to use. The output results are so spectacular, it's almost as if we were seeing magic unfold before our eyes. The fact that we have so many hairstyles in this dataset also opens up the possibility of editing, which is always quite a treat for artists working in the industry. The main limitation is a poorer reconstruction of regions that are not visible in the input photograph, but I think that goes without saying. It is slightly ameliorated by the fact that the public repositories contain hairstyles that make sense, so we can expect results of reasonable quality even for the regions we haven't seen in the input photograph. As always, please let me know below in the comments section whether you have found everything understandable in this episode. Was it easy to digest? Was there something that was not easy to follow? Your feedback, as always, is greatly appreciated. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "WaveNet by Google DeepMind",
            "url": "https://www.youtube.com/watch?v=CqFIVCD1WWo",
            "date": "2016-09-12",
            "tags": [],
            "papers": ["https://drive.google.com/file/d/0B3cxcnOkPx9AeWpLVXhkTDJINDQ/view"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. When I opened my inbox today, I was greeted by a huge deluge of messages about WaveNet. Well, first, it's great to see that so many people are excited about these inventions, and second, may all your wishes come true as quickly as this one! So here we go. This piece of work is about generating audio waveforms for Text To Speech and more. Text To Speech basically means that we have a voice reading whatever we have written down. The difference in this work, is, however that it can synthesize these samples in someone's voice provided that we have training samples of this person speaking. It also generates waveforms sample by sample, which is particularly perilous because we typically need to produce these at the rate of 16 or 24 thousand samples per second, and as we listen to the TV, radio and talk to each other several hours a day, the human ear and brain is particularly suited to processing this kind of signal. If the result is off by only the slightest amount, we immediately recognize it. It is not using a recurrent neural network, which is typically suited to learn sequences of things, and is widely used for sound synthesis. It is using a convolutional neural network, which is quite surprising because it is not meant to process sequences of data that change in time. However, this variant contains an extension that is able to do that. They call this extension dilated convolutions and they open up the possibility of making large skips in the input data so we have a better global view of it. If we were working in computer vision, it would be like increasing the receptive field of the eye so we can see the entire landscape, and not only a tree on a photograph. It is also a bit like the temporal coherence problem we've talked about earlier. Taking all this into consideration results in more consistent outputs over larger time scales, so the technique knows what it had done several seconds ago. Also, training a convolutional neural network is a walk in the park compared to a recurrent neural network. Really cool! And the results beat all existing widely used techniques by a large margin. One of these is the concatenative technique, which builds sentences from a huge amount of small speech fragments. These have seen a ton of improvements during the years, but the outputs are still robotic and it is noticeable that we're not listening to a human but a computer. The DeepMind guys also report that: &quot;Notice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model.&quot; At the same time, I'd like to note that in the next few episodes, it may be that my voice is a bit different, but don't worry about that. It may also happen that I am on a vacation but new episodes and voice samples pop up on the channel, please don't worry about that either. Everything is working as intended! They also experimented with music generation, and the results are just stunning. I don't know what to say. These difficult problems, these impenetrable walls crumble one after another as DeepMind takes on them. Insanity. Their blog post and the paper are both really well written, make sure to check them out, they are both linked in the video description box. I wager that artistic style transfer for sound and instruments is not only coming, but it'll be here soon. I imagine that we'll play a guitar and it will sound like a harp, and we'll be able to sing something in Lady Gaga's voice and intonation. I've also seen someone pitching the idea of creating audiobooks automatically with such a technique. Wow. I travel a lot and am almost always on the go, so I personally would love to have such audiobooks! I have linked the mentioned machine learning reddit thread in the description box, as always, there's lots of great discussion and ideas there. It was also reported that the algorithm currently takes 90 minutes to synthesize one second of sound waveforms. You know the drill, one followup paper down the line, it will take only a few minutes, a few more papers down the line, it'll be real time. Just think about all these advancements. What a time we're living in! And I am extremely excited to present them all to you Fellow Scholars in Two Minute Papers. Make sure to leave your thoughts and ideas in the comments section, we love reading them! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Estimating Matrix Rank With Neural Networks",
            "url": "https://www.youtube.com/watch?v=bLFISzfQCDQ",
            "date": "2016-09-16",
            "tags": [],
            "papers": ["http://www.oneweirdkerneltrick.com/rank.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. This piece of work is not meant to be a highly useful application, only a tongue in cheek jab at the rising trend of trying to solve simple problems using deep learning without carefully examining the problem at hand. As always, we note that all intuitive explanations are wrong, but some are helpful, and the most precise way to express these thoughts can be done by using mathematics. However, we shall leave that to the textbooks and will try to understand these concepts by floating about on the wings of intuition. In mathematics, a matrix is a rectangular array in which we can store numbers and symbols. Matrices can be interpreted in many ways, for instance, we can think of them as transformations. Multiplying a matrix with a vector means applying this transform to the vector, such as scaling, rotation or shearing. The rank of a matrix can be intuitively explained in many ways. My favorite intuition is that the rank encodes the information content of the matrix. For instance, in an earlier work on Separable Subsurface Scattering, we recognized that many of these matrices that encode light scattering inside translucent materials, are of relatively low rank. This means that the information within is highly structured and it is not random noise. And from this low rank property follows that we can compress and represent this phenomenon using simpler data structures, leading to an extremely efficient algorithm to simulate light scattering within our skin. However, the main point is that finding out the rank of a large matrix is an expensive operation. It is also important to note that we can also visualize these matrices by mapping the numbers within to different colors. As a fun sidenote, the paper finds, that the uglier the colorscheme is, the better suited it is for learning. This way, after computing the ranks of many matrices, we can create a lot of input images and output ranks for the neural network to learn on. After that, the goal is that we feed in an unknown matrix in the form of an image, and the network would have to guess what the rank is. It is almost like having an expert scientist unleash his intuition on such a matrix, much like a fun guessing game for intoxicated mathematicians. And the ultimate question, as always is, how does this knowledge learned by the neural network generalize? The results are decent, but not spectacular, but they also offer some insights as to which matrices have surprising ranks. We can also try computing the products of matrices, which intuitively translates to guessing the result after we have done one transformation after the other. Like the output of scaling after a rotation operation. They also tried to compute the inverse of matrices, for which the intuition can be undoing the transformation. If it is a rotation to a given direction, the inverse would be rotating back the exact same amount, or if we scaled something up, then scaling it back down would be its inverse. Of course, these are not the only operations that we can do with matrices, we only used these for the sake of demonstration. The lead author states on his website that this paper shows that &quot;linear algebra can be replaced with machine learning&quot;. Talk about being funny and tongue in cheek. Also, I have linked the website of David in the description box, he has a lot of great works and I am surely not doing him justice by of all those great works, covering this one. Rufus von Woofles, graduate of the prestigious Muddy Paws University was the third author of the paper, overlooking the entirety of the work and making sure that the quality of the results is impeccable. As future work, I would propose replacing the basic mathematical operators such as addition and multiplication by machine learning. Except that it is already done and is hilariously fun, and it even supports division by zero. Talk about the almighty powers of deep learning. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Sound Propagation With Adaptive Impulse Responses",
            "url": "https://www.youtube.com/watch?v=Mx8viOFKiIs",
            "date": "2016-09-19",
            "tags": [],
            "papers": ["http://gamma.cs.unc.edu/ADAPTIVEIR/paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Have you ever wondered how your voice, or your guitar would sound in the middle of a space station? A realistic simulation of sounds within virtual environments dramatically improves the immersion of the user in computer games and virtual reality applications. To be able to simulate these effects, we need to compute the interaction between sound waves and the geometry and materials within the scene. If you remember, we also had quite a few episodes about light simulations, where we simulated the interaction of light rays or waves and the scene we have at hand. Sounds quite similar, right? Well, kind of, and the great thing is that we can reuse quite a bit of this knowledge and some of the equations for light transport for sound. This technique we call path tracing, and it is one of the many well-known techniques used for sound simulation. We can use path tracing to simulate the path of many waves to obtain an impulse response, which is a simple mathematical function that describes the reverberation that we hear if we shoot a gun in a given scene, such as a space station or a church. After we obtained these impulse reponses, we can use an operation called the convolution with our input signal, like our voice to get a really convincing result. We have talked about this in more detail in earlier episodes, I've put a link for them in the video description box. It is important to know that the impulse reponse depends on the scene and where we, the listeners are exactly in the scene. In pretty much every concert ever, we find that sound reverberations are quite different in the middle of the arena versus standing at the back. One of the main contributions of this work is that it exploits temporal coherence. This means that even though the impulse response is different if we stand at different places, but these locations don't change arbitrarily and we can reuse a lot of information from the previous few impulse reponses that we worked so hard to compute. This way, we can get away with tracing much fewer rays and still get high-quality results. In the best cases, the algorithm executes five times as quickly as previous techniques, and the memory requirements are significantly more favorable. The paper also contains a user study. Limitations include a bit overly smooth audio signals and some fidelity loss in the lower frequency domains. Some of these scenes in the footage showcase up to 24 distinct sound sources, and all of them are simulated against the geometry and the materials found in the scene. So let's listen together and delight in these magnificent results. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "3D Printing Auxetic Materials",
            "url": "https://www.youtube.com/watch?v=5-xMV3sT3Tw",
            "date": "2016-09-30",
            "tags": [],
            "papers": ["http://lgg.epfl.ch/publications/2016/BeyondDevelopable/paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We are back! And in this episode, we shall talk about auxetic materials. Auxetic materials are materials that when stretched, thicken perpendicular to the direction we're stretching them. In other words, instead of thinning, they get fatter when stretched. Really boggles the mind, right? They are excellent at energy absorption and resisting fracture, and are therefore widely used in body armor design, and I've read a research paper stating that even our tendons also show auxetic behavior. These auxetic patterns can be cut out from a number of different materials, and are also used in footwear design and actuated electronic materials. However, all of these applications are restricted to rather limited shapes. Furthermore, even the simplest objects, like this sphere cannot be always approximated by inextensible materials. However, if we remove parts of this surface in a smart way, this inextensible material becomes auxetic, and can approximate not only these rudimentary objects, but much more complicated shapes as well. However, achieving this is not trivial. If we try the simplest possible solution, which would basically be shoving the material onto a human head like a paperbag, but as it is aptly demonstrated in these images, it would be a fruitless endeavor. This method tries to solve this problem by flattening the target surface with an operation that mathematicians like to call a conformal mapping. For instance, the world map in our geography textbooks is also a very astutely designed conformal mapping from a geoid object, the Earth, to a 2D plane which can be shown on a sheet of paper. However, this mapping has to make sense so that the information seen on this sheet of paper actually makes sense in the original 3D domain as well. This is not trivial to do. After this mapping, our question is where the individual points would have to be located so that they satisfy three conditions: one: the resulting shape has to approximate the target shape, for instance, the human head, as faithfully as possible two: the construction has to be rigid three: when we stretch the material, the triangle cuts have to make sense and not intersect each other, so huge chasms and degenerate shapes are to be avoided. This work is using optimization to obtain a formidable solution that satisfies these constraints. If you remember our earlier episode about optimization, I said there will be a ton of examples of that in the series. This is one fine example of that! And the results are absolutely amazing - the possibility of creating a much richer set of auxetic material designs is now within the realm of possibility, and I expect that it will have applications from designing microscopic materials, to designing better footwear and leather garments. And we are definitely just scratching the surface! The method supports copper, aluminum, plastic and leather designs, and I am sure there will be mind blowing applications that we cannot even fathom so early in the process. As an additional selling point, the materials are also reconfigurable, meaning that from the same piece of material, we can create a number of different shapes. Even non-trivial shapes with holes, such as a torus, can be created. Note that in mathematics, the torus is basically a fancy name for a donut. A truly fantastic piece of work, definitely have a look at the paper, it has a lot of topological calculations, which is an awesome subfield of mathematics. And, the authors' presentation video is excellent, make sure to have a look at that. Let me know if you have found this episode understandable, we always get a lot of awesome feedback and we love reading your comments. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Sound Synthesis for Fluids With Bubbles",
            "url": "https://www.youtube.com/watch?v=kwqme8mEgz4",
            "date": "2016-10-04",
            "tags": [],
            "papers": ["http://www.cs.cornell.edu/projects/Sound/bubbles/bubbles.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have had quite a few episodes on simulating the motion of fluids and creating beautiful footages from the results. In this work, the authors created a simulator, that shows us not only the motion of a piece of fluid, but the physics of bubbles within as well. This sounds great, but there are two huge problems: one, there are a lot of them, and two, they can undergo all kinds of deformations and topology changes. To conjure up video footage that is realistic, and relates to the real world, several bubble-related effects, such as entrainment, splitting, merging, advection and collapsing, all have to be simulated faithfully. However, there is a large body of research out there to simulate bubbles, and here, we are not only interested in the footage of this piece of fluid, but also what kind of sounds it would emit when we interact with it. The result is something like this. The vibrations of a bubble is simulated by borrowing the equations that govern the movement of springs in physics. However, this, by itself would only be a forlorn attempt at creating a faithful sound simulation, as there are other important factors to take into consideration. For instance, the position of the bubble matters a great deal. This example shows that the pitch of the sound is expected to be lower near solid walls, as you can see it marked with dark blue on the left, right side and below, and have a higher pitch near the surface, which is marked with red. You can also see that there are significant differences in the frequencies depending on the position, the highest frequency being twice as high as the lowest. So this is definitely an important part of the simulation. Furthermore, taking into consideration the shape of the bubbles is also of utmost importance. As the shape of the bubble goes from an ellipsoid to something close to a sphere, the emitted sound frequency can drop by as much as 30%. Beyond these effects, there were still blind spots even in state of the art simulations. With previous techniques, a chirp-like sound was missing, which is now possible to simulate with a novel frequency extension model. Additional extensions include a technique that models the phenomenon of the bubbles popping at the surface. The paper discusses what cases are likely to emphasize which of these extension's effects. Putting it all together, it sounds magnificent, check it out! But still, however great these sounds are, without proper validation, these are still just numbers on a paper. And of course, as always, the best way of testing these kinds of works if we let reality be our judge, and compare the results to real world footage. So I think you can guess what the next test is going to be about! The  authors also put up a clinic on physics and math, and the entirety of the paper is absolutely beautifully written. I definitely recommend having a look, as always, the link is available in the description box. Also, you'll find one more link to a playlist with all of our previous episodes on fluid simulations. Lots of goodies there. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "3D Printing Materials With Subsurface Scattering",
            "url": "https://www.youtube.com/watch?v=w2D5JR83pFI",
            "date": "2016-10-09",
            "tags": [],
            "papers": ["http://people.csail.mit.edu/wojciech/PRO/PRO.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Subsurface scattering means that not every ray of light is reflected or absorbed on the surface of a material, but some of it may get inside somewhere, and come out somewhere else. For instance, our skin is a great and fairly unknown example of that. We can witness this beautiful effect if we place a strong light source behind our ears. Note that many other materials, such as plant leaves, many fruits such as apples and oranges, wax, marble also have subsurface scattering. The more we look at objects like these, the more we recognize how beautiful and ubiquitous subsurface scattering and translucency is in mother nature. And today, our main question is whether we can reproduce this kind of effect with 3D printed materials. The input would be a real material, such as these slabs, and the output would be an arbitrary shaped 3d printed material with similar scattering properties. Something that looks similar. What you see here is already the result of the 3D printing process, and wow, they look very tasty indeed. The process starts with a measurement apparatus where we grab a real material, and create a diffusion profile from it that describes how light scatters inside of this material. We have talked quite a bit about diffusion profiles before, I've put some links to earlier episodes in the video description box. If you check it out, you'll see how we can add subsurface scattering to an already existing image by &quot;kind of&quot; multiplying it with an other image. This is another one of those amazing inventions of mankind. Now, onto 3D printing. When we would like to 3d print something, we basically have a few different materials to work with, and we have to specify a shape. This shape is approximated with a three-dimensional grid. Each of these tiny grid elements typically have the thickness of several microns, which basically means a tiny fraction of the diameter of one hair strand, and we like to call these elements voxels. Now, before printing, we have to specify what kind of material we'd like to fill each of these voxels with. This is the general workflow for most 3D printers. What is specific to this work is that, after that, we have to take one column of this material, and look at the scattering properties of it. Let's call this column one stacking. We could measure that stacking by hand and see how it relates to the original target material, and we are trying to minimize the difference between the two. However, it would take millions of tries and would likely take a lifetime to print just one high-quality reproduction. So basically, we have an optimization problem where we're looking for a stacking that will appear similar to the chosen diffusion profiles. The difference between the appearance of the two is to be minimized. However, we have to realize, that in physics, the laws of light scattering are well understood, and the wonderful thing is that instead of printing a real object, we could just use a light simulation program to tell us how close the results should be. Now, this would work great, but it would still take an eternity because simulating light scattering through a stack of materials would take the very least, several seconds. And we have to try up to millions of stackings for each column, and there is a lot of columns to compute. Why a lot of different columns? Well, it's because we have a heterogeneous problem, which means that the whole material can contain variations in color and scattering properties. The geometry may also be uneven, so this is a vastly more difficult formulation of the initial problem. A classical light simulation program would be able to solve this, well, in a matter of years. However, there is a wonderful tool that is able to almost immediately tell us how much light is scattering inside of a stack of a ton of different materials. An almost instant multi-layer scattering tool, if you will. It really is a miracle that we can get the results for something so quickly that would otherwise require following the paths of millions of light rays. We call this technique the Hankel transform. The mathematical description of it is absolutely beautiful, but I personally think the best way of motivating these techniques is through application. Like this one. Imagine that many mathematicians have to study this transform without ever hearing what it can be used for. These are not some dry and tedious materials that one has to memorize - we can do miracles with these inventions, and I feel that people need to know about that! With the use of the Hankel transform and some additional optimizations, one can efficiently find solutions that lead to high-quality reproductions of the input material. Excellent piece of work, definitely one of my favorites in 3D fabrication. As always, we'd love to read your feedback on this episode, let us know whether you have found it understandable! I hope you did! Also, a quick shoutout to BetterExplained.com. Please note that this is not a sponsored message. It has multiple slogans, such as &quot;math lessons for lasting insight&quot; or &quot;math without endless memorization&quot;. This webpage is run by Kalid Azad, and contains tons of intuitive math lessons I wish I had access to during my years at the university. For instance, here is his guide on Fourier Transforms, which is a staple technique in every mathematician's and engineer's skill set, and is a prerequisite to understanding the Hankel transform. If you wish to learn mathematics, definitely check this website out, and if you don't wish to learn mathematics, then also definitely check this website out. Thanks for watching, and for your generous  support, and I'll see you next time!"
        },
        {
            "name": "MeshGit: Version Control for Meshes",
            "url": "https://www.youtube.com/watch?v=p-u2NZ4vUxI",
            "date": "2016-10-12",
            "tags": [],
            "papers": ["http://pellacini.di.uniroma1.it/publications/meshgit13/meshgit13-paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Version control tools are excellent in helping software developers track changes and collaborate in their projects. This usually entails having a look at files with source code, that can be compiled into machine code that a computer can execute. A diff, is an assessment of the difference between a previous and the current version a file. Text files are reasonably easy to diff, we just list what has been removed, added, and changed recently. This is very helpful if there are multiple people working on the same piece of code and would like to push their changes into the main program. The developers have local copies of this computer code and work independently from each other. However, it can easily happen that the same file was changed by multiple people without knowing about each other's work. This way, we can attempt a merging operation that sorts these kinds of conflicts out. Many developers have recurring nightmares about resolving these merge conflicts. After these conflicts are resolved, everyone who is interested can immediately access the new version of the software. Git is one of these version control systems developed by Linus Torvalds, the principal developer of the Linux kernel. The question this work is interested in is can we do diffs on digital 3d models? These 3d models we define as a set of points. From the points we can create triangles, and from a set of triangles, we have a triangle mesh, which is suitable for the representation and rendering of digital objects and characters. But have a look at this example - what has changed here? Well, legs were added, that's for sure. The teeth did undergo some changes, and the thighs as well. But if we would like to get a proper assessment on what has changed, we have to look for quite a while. In computer graphics, we like to call a point a vertex, and points we refer to as vertices. The first thing that comes to mind is to highlight the changes of vertex positions. This point went here, that point went there. Simple enough. Except that it won't be useful at all. Have a look at this example here. What has changed? Well, the carbohydrate intake, I would say. Jokes aside, the position of every vertex has changed. It's like going to the doctor, where he asks, &quot;where does it hurt&quot;? And the answer is, &quot;everywhere&quot;. That's not such a useful assessment, is it? Do you remember these puzzles for children where one has to circle the differences between two images? Lots of fond memories, right? Well, not so much for mathematicians, because they always flip out and ask &quot;what do we really consider to be a difference&quot;? Mathematically, every vertex position is different here, but we could say that the stature of the lady and parts of her clothing has changed. That is a much more intuitive assessment. We have to come up with something better to measure the similarity or dissimilarity between different models. This has to be something different than simple differences in vertex positions. And in this piece of work, the basic idea is to compute the minimal distance between the two models, but this distance we define as the minimal amount of mesh editing operations that could yield the second model from the first one. These operations are implemented in many modeling software, for instance, we can add, remove vertices, extrude, rotate, or scale different faces. And now, we can see what Meshgit thinks about these changes. Very intuitive indeed, right? So, the mesh is the digital 3d model, and git is a version control system. And, there you go, meshgit! This technique has been implemented in Maya, an excellent modeling program that is widely used in the motion picture industry. I really love this kind of research where the results can be quickly implemented into state of the art software and used almost right away. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "How Do Neural Networks See The World?",
            "url": "https://www.youtube.com/watch?v=hBobYd8nNtQ",
            "date": "2016-10-16",
            "tags": [],
            "papers": ["http://www.evolvingai.org/files/mfv_icml_workshop_16.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Today we are here to answer one question: how to visualize individual neurons in a neural network? These networks are capable of amazing things: they can learn the laws of fluid dynamics, describe in a full sentence what is seen on an image, beat many of the state of the art works in cancer research and more. Before we start, during this episode, if there is anything that does not sound familiar, I placed some helper captions in this video, and also some links to our earlier episodes on neural networks and many of these applications in the video description box. So let's say we are interested in recognizing human faces. Now, laying down the mathematical model of what a human face is would not only be a mortal sin, but it would likely be a fruitless effort as well. Every single one of us can learn to recognize a human face, but how can one explain to a machine what it is exactly? So what a joyous opportunity this is to train a neural network on a database of human faces. If we do that, the first inner layer of this neural network will likely contain edges. These edges are a combination of the pixels of the input image. As we are talking about deep learning, what we have at hand is a deep neural network that is equipped with several inner layers. Looking into the next layer, we'd see combinations of these edges, which make object parts, and one layer deeper, a combination of object parts yield object models. As it is demonstrated quite aptly on this image, approaching the later layers, we get greeted by more and more sophisticated patterns. If we are to visualize what a neuron has learned, we can take a look at some results for a convolutional neural network that I trained to recognize wooden patterns. In the first layer, it is looking for colors, then in the second layer, some basic patterns emerge. As we look into the third layer, we see that it starts to recognize horizontal, vertical and diagonal patterns, and in the fourth and fifth layers, it uses combinations of the previously seen features, and as you can see, beautiful, symmetric figures emerge. The question immediately arises - how are these visualizations created? One cannot just simply visualize a neuron like this, because all it contains is a bunch of connections to the previous layer and something that we call a tiny convolutional filter. None of these are immediately useful to visualize. But what we are seeing here are images, so some witchcraft has been done. How is this done exactly? And now, buckle up, because a really cool technique follows. What we can do to visualize what a neuron gets excited about, is, at first, showing it a completely random noisy image. Then, we slightly modify this image and look at the activations within our neuron of choice. After this, in every round, we try to modify this image to maximize the amount of activations in this chosen neuron. At the end of this process, we will hopefully converge to some result that makes this given particular neuron as excited as possible. This is also an optimization problem solved via gradient descent. If you don't know what gradient descent is, I've put a link to our episode on it in the video description box, check it out! And this is a really fun process, I've run this optimizer several times on the neural network that learned on the wooden dataset and visualized how this exciting image evolves over time. This is a modified version of Francois Chollet's code using the Keras library, which is shown here. If you are interested in programming, give it a try, it's a lot of fun. So this thing, where we try to send impulses through the network and trying to maximize the amount of activations in a given neuron, as you might have guessed, we like to call activation maximization. Really cool! However, we can do even better than this using this novel technique that the we shall call multifaceted feature visualization. And now is the point where we arrive to today's paper. The basic idea is that neurons are not meant to recognize only one type of feature, but are multifaceted. Now, what does this mean? A fine example of this is when there is a designated neuron for detecting groceries, but this means that they should be activated not only when one piece of vegetable is shown, but also when an entire row of goods appear on the image, but even for a huge storefront as well. In short, this kind of visualization shows not only one possible image that these neurons are excited for, but all kinds of images, that are also more relevant. In this paper, the authors promise a more comprehensive visualization of each neuron's role, and boy, do they deliver! Here you see a multifaceted visualization of this grocery neuron, and inputs that make this tiny little neuron extremely excited. And in this example, you can see a visualization of several neurons in a deep convolutional neural network from layers number one to eight. For each neuron, four possible facets are shown. What a beautiful sight! I think the paper is an absolutely amazing read because it formalizes some really exciting problems, and if you are unfamiliar with dimensionality reduction, t-SNE visualization, k-means clustering, and all that wizardry, you can learn about all of these techniques in one go, which is an amazing deal, so have a look at the description box and read the living hell out of it! There are also a ton more visualization examples therein as well, I think this is quite fascinating for everyone, not only for scientists. Give it a try! We know that deep neural networks are tremendously useful, we know exactly how to train them, but after the training process, the way the neural network works is often is shrouded in obsessive secrecy. Every piece of work that makes us better understand what is happening under the hood in these neural networks is more than welcome, and I think this is one of the exciting future frontiers in machine learning research. One of many, if I may add! And before we go, a quick shoutout to the YouTube channel by the name Sirajology. Siraj has a ton of deep learning tutorials, most of them involving coding cool systems to generate music, building simulations, neural networks, self driving cars, and more. You'll see a lot of concepts that we have talked about come to life in these tutorials. I am sure you'll love it, check it out! As always, the link awaits you in the description box. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Image Editing with Generative Adversarial Networks",
            "url": "https://www.youtube.com/watch?v=pqkpIfu36Os",
            "date": "2016-10-22",
            "tags": [],
            "papers": ["https://arxiv.org/pdf/1609.03552v2.pdf"],
            "implementations": ["https://github.com/junyanz/iGAN"],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. In a previous episode, we mentioned the YouTube channel of Siraj, if you happened to miss it, make sure to have a look for a bunch of great coding lessons on deep learning. Today, we're going to talk about image editing. There are a lot of techniques to edit a photograph in different artistic ways, but oftentimes, there is absolutely nothing that prevents novice users from creating something that some people like to call a Photoshop disaster. Many of these image editing tools would be much more useful and realistic with techniques that have some degree of understanding of the image we're editing. It would be really cool to have an algorithm that reviews and adjusts our edits to remain realistic. How can we achieve this? We could technically train a neural network to be able to draw new images that make sense. This is still far away from our initial task, but it would likely be a decent start. However, this would require millions of training examples, so we would need an immensely patient guy to look at these initially horrible images, tell the network that they don't look real at all, and the network would learn from these examples. In time, this network would gather enough feedback to be able to generate better and better images that we, humans, believe to be real photographs. So this idea is great, but it's not in the realm of possibility because of the immense amount of human labor required for it. And now comes the brilliant idea. What if instead of this immensely patient guy, we would plug in another neural network to judge these works. The drawing neural network would be called the generative network, and the immensely patient neural network guy would be called the discriminator network. The generative network creates new images all day, and the discriminator network is also available all day to judge whether they look natural or not. During this process, the generator network learns to draw more realistic images, and the discriminator network learns to tell fake images from real ones. They drive each other, and they learn from each other. This is such an insane idea, and it's probably needless to say, I love it! And that is the concept of Generative Adversarial Networks (GANs in short). Nowadays, they are seeing more and more use in image generation, and now, image editing as well. So what are the applications of this explored in the paper? We take an input image, and change either the color or warp the shape of it, and the generative adversarial network will create something that is as close to our envisioned result as possible, but is still realistic. It has another mode, where we start out from an empty image, and draw the outline of a mountain, maybe add a blue sky above with a simple brush stroke, and of course, the mandatory snow. We can also perform morphing or interpolation between an input and a target shape as well, and the goal is that each of these individual images produced during this process are realistic. The results are compared to the interpolation method that is deployed in Adobe Premiere Pro. The results speak for themselves - my mind is blown. The flowers of deep learning are blooming indeed. We can also get interactive feedback as we draw our desired shape.The code and data to reproduce the experiments are both available online. The authors ran their technique on an NVIDIA TITAN X video card, which was able to produce results interactively. Creating finer shapes and textures with this method should definitely be a promising direction for future work. I feel that generative adversarial networks will see a ton of use in the upcoming years and I am looking forward to having a ton of fun coding up networks like this. More to come on this topic in Two Minute Papers as well, stay tuned! Also, some of our episodes are already available in three languages. How cool is that? A huge thanks for everyone who contributed! You Fellow Scholars are the best! Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Generating Tangle Patterns With Grammars",
            "url": "https://www.youtube.com/watch?v=nK3giIsNAHg",
            "date": "2016-10-26",
            "tags": [],
            "papers": ["http://pellacini.di.uniroma1.it/publications/gtangle16/gtangle16-paper.pdf", "https://www.cg.tuwien.ac.at/research/publications/2015/Ilcik_2015_LAY/Ilcik_2015_LAY-preprint.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. A tangle pattern is a beautiful, intervowen tapestry of basic stroke patterns, like dots, straight lines, and simple curves. If we look at some of these works, we see that many of these are highly structured, and maybe, we could automatically create such beautiful structures with a computer. And, now, hold on to your papers, because this piece of work is about generating tangle patterns with grammars. Okay, now, stop right there. How on Earth do grammars have anything to do with computer graphics or tangle patterns? The idea of this sounds as outlandish as it gets. Grammars are a set of rules that tell us how to build up a structure, such as a sentence properly from small elements, like nouns, adjectives, pronouns, and so on. Math nerds also study grammars extensively and set up rules that enforce that every mathematical expression satisfies a number of desirable constraints. It's not a surprise that when mathematicians talk about grammars, they will use these mathematical hieroglyphs like the ones you see on the screen. It is a beautiful subfield of mathematics that I have studied myself before, and am still hooked. Especially given the fact that from grammars, we can build not only sentences, but buildings. For instance, a shape grammar for buildings can describe rules like a wall can contain several windows, below a window goes a window sill, one wall may have at most two doors attached, and so on. My friend Martin Ilcik is working on defining such shape grammars for buildings, and using these grammars, he can generate a huge amount of different skyscrapers, facades, and all kinds of cool buildings. In this piece of work, we start out with an input shape, subdivide it into multiple other shapes, assign these smaller shapes into groups. And the final tangle is obtained by choosing patterns and assigning them to all of these groups. This yields a very expressive, powerful, tool that anyone can use to create beautiful tangle patterns. And all this, through the power of grammars. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "Real-Time Soft Body Dynamics for Video Games",
            "url": "https://www.youtube.com/watch?v=fl-7e8yBUic",
            "date": "2016-10-29",
            "tags": [],
            "papers": ["http://pellacini.di.uniroma1.it/publications/vivace16/vivace16-paper.pdf"],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. We have had plenty of episodes about fluid simulations, so how about some tasty soft body dynamics for today? Soft body dynamics basically means computing what happens when we smash together different deformable objects. Examples include folding sheets, playing around with noodles, or torturing armadillos. I think this is a nice and representative showcase of the immense joys of computer graphics research! The key to real-time physically based simulations is parallelism. Parallelism means that we have many of the same units working together in harmony. Imagine if we had to assign 50 people to work together to make a coffee in the same kitchen. As you may imagine, they would trip over each other, and the result would be chaos, not productivity. Such a process would not scale favorably, because as we would add more people after around 3 or 4, the productivity would not increase, but drop significantly. You can often hear a similar example of 9 pregnant women not being able to give birth to a baby in one month. For better scaling, we have to subdivide a bigger task into small tasks in a way that these people can work independently. The more independently they can work, the better the productivity will scale as we add more people. In software engineering, these virtual people we like call threads, or compute units. As of 2016, mid-tier processors are equipped with 4-8 logical cores, and for a video card, we typically have compute units in the order of hundreds. So if we wish to develop efficient algorithms, we have to make sure that these big simulation tasks are subdivided in a way so that these threads are not tripping over each other. And the big contribution of this piece of work is a technique to distribute the computation tasks to these compute units in a way that they are working on independent chunks of the problem. This is achieved via using graph coloring, which is a technique typically used for designing seating plans, exam timetabling, solving sudoku puzzles and similar assignment tasks. It not only works in an absolutely spectacular manner, but graph theory is an immensely beautiful subfield of mathematics, so additional style points to the authors! The technique produces remarkably realistic animations and requires only 15 milliseconds per frame, which means that this technique can render over 60 frames per second comfortably. And the other most important factor is that this technique is also stable, meaning that it offers an appropriate solution, even when many other techniques fail to deliver. Thanks for watching, and for your generous support, and I'll see you next time!"
        },
        {
            "name": "How To Steal a Lost Election With Gerrymandering",
            "url": "https://www.youtube.com/watch?v=u9kvJbWb_1U",
            "date": "2016-11-2",
            "tags": [],
            "papers": [],
            "implementations": [],
            "transcript": "Dear Fellow Scholars, this is Two Minute Papers with Károly Zsolnai-Fehér. Let's talk about the mathematical intricacies of the elections! Here you can see the shape of the twelfth congressional district in North Carolina in the 90's. This is not a naturally shaped electoral district, is it? One might say this is more of an abomination. If we try to understand why it has this peculiar shape, we shall find a remarkable mathematical mischief. Have a look at this example of 50 electoral precincts. The distribution is 60% percent blue, and 40% red. So this means that the blue party should win the elections and gain seats with the ratio of 60 to 40, right? Well, this is not exactly how it works. There is a majority decision district by district, regardless of the vote ratios. If the electoral districts are shaped like this, then the blue party wins 5 seats to zero. However, if they are, instead, shaped like this, the red party wins 3 to 2. Which is kind of mind blowing, because the votes are the very same. And this is known as the wasted vote effect. This term doesn't refer to someone who enters the voting booth intoxicated, this means that one can think of pretty much every vote beyond 50% + 1 to a party in a district, to be irrelevant. It doesn't matter if the district is won by 99% of the votes or just by 50% + 1 vote. So, the cunning plan is now laid out. What if, instead, we could regroup all these extra votes to win in a different district where we were losing? And now, we have ceremoniously arrived to the definition of Gerrymandering, which is the process of manipulating electoral district boundaries to turn the tide of an election. The term originates from one of the elections in the USA in the 1800s, where Governor Elbridge Gerry signed a bill to reshape the districts of Massachusetts in order to favor his party. And at that time, understanably, all the papers and comic artists were up in arms about this bill. So how does one perform gerrymandering? Gerrymandering is actually a mathematical problem of the purest form where we are trying to maximize the number of seats that we can win by manipulating the district boundaries appropriately. It is important to note that the entire process relies on a relatively faithful prediction of the vote distributions per region, which in many countries, is not really changing all that much in time. This is a problem that we can solve via standard optimization techniques. Now, hold on to your papers, and get this: for instance, we can use Metropolis sampling to solve this problem, which is, absolutely stunning. So far, in an earlier episode, we have used Metropolis sampling to develop a super efficient light simulation program to create beautiful images of virtual scenes, and the very same technique can also be used to steal an election. In fact, Metropolis sampling was developed and used during the Manhattan project, where the first atomic bomb was created in Los Alamos. I think it is completely understandable that the power of mathematics and research still give many of us sleepless nights, sometimes delightful, sometimes perilous. It is also important to note that in order to retain the fairness of the elections in a district-based system, it is of utmost importance that these district boundaries are drawn by an independent organization and that the process is as transparent as possible. I decided not to cite a concrete paper in this episode. If you would like to read up on this topic, I recommend searching for keywords, like redistricting and gerrymandering on Google Scholar. Please feel free to post the more interesting finding of yours in the comments section, we always have excellent discussions therein. Thanks for watching, and for your generous support, and I'll see you next time!"
        }
    ]
}
